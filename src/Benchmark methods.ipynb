{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate user data will be skipped\n",
      "[0, 2]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['../data/augment_data_from_csvs/',\n",
    "          '../data/augment_data_from_csvs/no_interm/',\n",
    "]\n",
    "\n",
    "outputs = ['../results/csv_based/',\n",
    "           '../results/csv_without_interms/',\n",
    "]\n",
    "\n",
    "ind = 1\n",
    "\n",
    "withoutInterms = (ind == 1)\n",
    "\n",
    "output_root = outputs[ind]\n",
    "input_root = inputs[ind]\n",
    "\n",
    "if withoutInterms:\n",
    "    print('Intermediate user data will be skipped')\n",
    "else:\n",
    "    print('All three user groups (Beginner, Intermediate, Expert) are processed')\n",
    "    \n",
    "groupRange = [0,2]\n",
    "if not withoutInterms:\n",
    "    groupRange = [0,1,2]\n",
    "print(groupRange)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "generateOnlyMissingResultFiles = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Suture_dataX_basic_90_30.mat  loaded\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DFT/3_Suture_90_30_1_report_DFT_fea.txt already exists\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Knot_dataX_basic_90_30.mat  loaded\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DFT/3_Knot_90_30_1_report_DFT_fea.txt already exists\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Needle_dataX_basic_90_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5092592592592593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.93      0.48        80\n",
      "         2.0       0.94      0.37      0.53       244\n",
      "\n",
      "    accuracy                           0.51       324\n",
      "   macro avg       0.63      0.65      0.51       324\n",
      "weighted avg       0.79      0.51      0.52       324\n",
      "\n",
      "[[ 74   6]\n",
      " [153  91]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5226130653266332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.63      0.66       292\n",
      "         2.0       0.18      0.23      0.20       106\n",
      "\n",
      "    accuracy                           0.52       398\n",
      "   macro avg       0.44      0.43      0.43       398\n",
      "weighted avg       0.56      0.52      0.54       398\n",
      "\n",
      "[[184 108]\n",
      " [ 82  24]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.5179372197309418\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.71      0.61       237\n",
      "         2.0       0.48      0.30      0.37       209\n",
      "\n",
      "    accuracy                           0.52       446\n",
      "   macro avg       0.51      0.50      0.49       446\n",
      "weighted avg       0.51      0.52      0.50       446\n",
      "\n",
      "[[169  68]\n",
      " [147  62]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.5974025974025974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.47      0.59       280\n",
      "         2.0       0.49      0.79      0.61       182\n",
      "\n",
      "    accuracy                           0.60       462\n",
      "   macro avg       0.63      0.63      0.60       462\n",
      "weighted avg       0.66      0.60      0.59       462\n",
      "\n",
      "[[132 148]\n",
      " [ 38 144]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5669515669515669\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.89      0.69       187\n",
      "         2.0       0.61      0.20      0.30       164\n",
      "\n",
      "    accuracy                           0.57       351\n",
      "   macro avg       0.59      0.54      0.49       351\n",
      "weighted avg       0.58      0.57      0.51       351\n",
      "\n",
      "[[166  21]\n",
      " [131  33]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Suture_dataX_basic_90_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5070707070707071\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.58      0.65       392\n",
      "         2.0       0.12      0.21      0.15       103\n",
      "\n",
      "    accuracy                           0.51       495\n",
      "   macro avg       0.43      0.40      0.40       495\n",
      "weighted avg       0.61      0.51      0.55       495\n",
      "\n",
      "[[229 163]\n",
      " [ 81  22]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5634328358208955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.76      0.71       185\n",
      "         2.0       0.19      0.12      0.15        83\n",
      "\n",
      "    accuracy                           0.56       268\n",
      "   macro avg       0.42      0.44      0.43       268\n",
      "weighted avg       0.51      0.56      0.53       268\n",
      "\n",
      "[[141  44]\n",
      " [ 73  10]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.6424418604651163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.71      0.75       266\n",
      "         2.0       0.30      0.42      0.35        78\n",
      "\n",
      "    accuracy                           0.64       344\n",
      "   macro avg       0.55      0.56      0.55       344\n",
      "weighted avg       0.69      0.64      0.66       344\n",
      "\n",
      "[[188  78]\n",
      " [ 45  33]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.546031746031746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.58      0.65       224\n",
      "         2.0       0.31      0.45      0.36        91\n",
      "\n",
      "    accuracy                           0.55       315\n",
      "   macro avg       0.51      0.52      0.51       315\n",
      "weighted avg       0.60      0.55      0.57       315\n",
      "\n",
      "[[131  93]\n",
      " [ 50  41]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.22878228782287824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.28      0.35       202\n",
      "         2.0       0.03      0.07      0.05        69\n",
      "\n",
      "    accuracy                           0.23       271\n",
      "   macro avg       0.25      0.18      0.20       271\n",
      "weighted avg       0.36      0.23      0.27       271\n",
      "\n",
      "[[ 57 145]\n",
      " [ 64   5]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Knot_dataX_basic_90_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.53      0.60       101\n",
      "         2.0       0.29      0.42      0.34        45\n",
      "\n",
      "    accuracy                           0.50       146\n",
      "   macro avg       0.48      0.48      0.47       146\n",
      "weighted avg       0.56      0.50      0.52       146\n",
      "\n",
      "[[54 47]\n",
      " [26 19]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.45255474452554745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.46      0.55        98\n",
      "         2.0       0.24      0.44      0.31        39\n",
      "\n",
      "    accuracy                           0.45       137\n",
      "   macro avg       0.46      0.45      0.43       137\n",
      "weighted avg       0.55      0.45      0.48       137\n",
      "\n",
      "[[45 53]\n",
      " [22 17]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.68125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.78      0.78       118\n",
      "         2.0       0.40      0.40      0.40        42\n",
      "\n",
      "    accuracy                           0.68       160\n",
      "   macro avg       0.59      0.59      0.59       160\n",
      "weighted avg       0.68      0.68      0.68       160\n",
      "\n",
      "[[92 26]\n",
      " [25 17]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6196319018404908\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.72      0.75       126\n",
      "         2.0       0.22      0.27      0.24        37\n",
      "\n",
      "    accuracy                           0.62       163\n",
      "   macro avg       0.50      0.50      0.49       163\n",
      "weighted avg       0.65      0.62      0.63       163\n",
      "\n",
      "[[91 35]\n",
      " [27 10]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.71      0.75       102\n",
      "         2.0       0.42      0.56      0.48        39\n",
      "\n",
      "    accuracy                           0.67       141\n",
      "   macro avg       0.62      0.63      0.62       141\n",
      "weighted avg       0.70      0.67      0.68       141\n",
      "\n",
      "[[72 30]\n",
      " [17 22]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Needle_dataX_basic_90_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.36809815950920244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.82      0.39        40\n",
      "         2.0       0.79      0.22      0.34       123\n",
      "\n",
      "    accuracy                           0.37       163\n",
      "   macro avg       0.52      0.52      0.37       163\n",
      "weighted avg       0.66      0.37      0.36       163\n",
      "\n",
      "[[33  7]\n",
      " [96 27]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.69      0.69       147\n",
      "         2.0       0.18      0.19      0.18        53\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.44      0.44      0.44       200\n",
      "weighted avg       0.56      0.56      0.56       200\n",
      "\n",
      "[[101  46]\n",
      " [ 43  10]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.3794642857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.50      0.46       119\n",
      "         2.0       0.30      0.24      0.26       105\n",
      "\n",
      "    accuracy                           0.38       224\n",
      "   macro avg       0.36      0.37      0.36       224\n",
      "weighted avg       0.37      0.38      0.37       224\n",
      "\n",
      "[[60 59]\n",
      " [80 25]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6137339055793991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.48      0.60       141\n",
      "         2.0       0.51      0.82      0.62        92\n",
      "\n",
      "    accuracy                           0.61       233\n",
      "   macro avg       0.65      0.65      0.61       233\n",
      "weighted avg       0.68      0.61      0.61       233\n",
      "\n",
      "[[68 73]\n",
      " [17 75]]\n",
      "User out:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data\n",
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.85      0.65        94\n",
      "         2.0       0.36      0.10      0.15        82\n",
      "\n",
      "    accuracy                           0.50       176\n",
      "   macro avg       0.44      0.47      0.40       176\n",
      "weighted avg       0.45      0.50      0.42       176\n",
      "\n",
      "[[80 14]\n",
      " [74  8]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Suture_dataX_basic_120_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.4882533197139939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.57      0.64       777\n",
      "         2.0       0.10      0.18      0.13       202\n",
      "\n",
      "    accuracy                           0.49       979\n",
      "   macro avg       0.41      0.37      0.38       979\n",
      "weighted avg       0.60      0.49      0.53       979\n",
      "\n",
      "[[442 335]\n",
      " [166  36]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5681818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.77      0.71       365\n",
      "         2.0       0.18      0.12      0.14       163\n",
      "\n",
      "    accuracy                           0.57       528\n",
      "   macro avg       0.42      0.44      0.43       528\n",
      "weighted avg       0.51      0.57      0.54       528\n",
      "\n",
      "[[281  84]\n",
      " [144  19]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.6338235294117647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.74      0.76       526\n",
      "         2.0       0.23      0.27      0.25       154\n",
      "\n",
      "    accuracy                           0.63       680\n",
      "   macro avg       0.50      0.50      0.50       680\n",
      "weighted avg       0.65      0.63      0.64       680\n",
      "\n",
      "[[390 136]\n",
      " [113  41]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.617741935483871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.65      0.71       442\n",
      "         2.0       0.38      0.53      0.44       178\n",
      "\n",
      "    accuracy                           0.62       620\n",
      "   macro avg       0.58      0.59      0.58       620\n",
      "weighted avg       0.66      0.62      0.63       620\n",
      "\n",
      "[[289 153]\n",
      " [ 84  94]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.40789473684210525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.48      0.55       397\n",
      "         2.0       0.11      0.19      0.14       135\n",
      "\n",
      "    accuracy                           0.41       532\n",
      "   macro avg       0.37      0.33      0.34       532\n",
      "weighted avg       0.50      0.41      0.44       532\n",
      "\n",
      "[[192 205]\n",
      " [110  25]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Knot_dataX_basic_120_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.7007042253521126\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.64      0.75       197\n",
      "         2.0       0.51      0.83      0.63        87\n",
      "\n",
      "    accuracy                           0.70       284\n",
      "   macro avg       0.70      0.74      0.69       284\n",
      "weighted avg       0.78      0.70      0.71       284\n",
      "\n",
      "[[127  70]\n",
      " [ 15  72]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5842696629213483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.64      0.69       192\n",
      "         2.0       0.33      0.45      0.38        75\n",
      "\n",
      "    accuracy                           0.58       267\n",
      "   macro avg       0.54      0.54      0.53       267\n",
      "weighted avg       0.63      0.58      0.60       267\n",
      "\n",
      "[[122  70]\n",
      " [ 41  34]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.5737179487179487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.67      0.70       231\n",
      "         2.0       0.25      0.31      0.27        81\n",
      "\n",
      "    accuracy                           0.57       312\n",
      "   macro avg       0.49      0.49      0.49       312\n",
      "weighted avg       0.61      0.57      0.59       312\n",
      "\n",
      "[[154  77]\n",
      " [ 56  25]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.71875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.79      0.81       248\n",
      "         2.0       0.40      0.47      0.43        72\n",
      "\n",
      "    accuracy                           0.72       320\n",
      "   macro avg       0.62      0.63      0.62       320\n",
      "weighted avg       0.74      0.72      0.73       320\n",
      "\n",
      "[[196  52]\n",
      " [ 38  34]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.6277372262773723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.71      0.73       199\n",
      "         2.0       0.35      0.41      0.38        75\n",
      "\n",
      "    accuracy                           0.63       274\n",
      "   macro avg       0.56      0.56      0.56       274\n",
      "weighted avg       0.65      0.63      0.64       274\n",
      "\n",
      "[[141  58]\n",
      " [ 44  31]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Needle_dataX_basic_120_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.3987538940809969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.85      0.41        79\n",
      "         2.0       0.84      0.25      0.39       242\n",
      "\n",
      "    accuracy                           0.40       321\n",
      "   macro avg       0.55      0.55      0.40       321\n",
      "weighted avg       0.70      0.40      0.39       321\n",
      "\n",
      "[[ 67  12]\n",
      " [181  61]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.48223350253807107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.63      0.64       289\n",
      "         2.0       0.08      0.09      0.08       105\n",
      "\n",
      "    accuracy                           0.48       394\n",
      "   macro avg       0.37      0.36      0.36       394\n",
      "weighted avg       0.50      0.48      0.49       394\n",
      "\n",
      "[[181 108]\n",
      " [ 96   9]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.40271493212669685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.59      0.51       235\n",
      "         2.0       0.29      0.19      0.23       207\n",
      "\n",
      "    accuracy                           0.40       442\n",
      "   macro avg       0.37      0.39      0.37       442\n",
      "weighted avg       0.38      0.40      0.38       442\n",
      "\n",
      "[[138  97]\n",
      " [167  40]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.5842450765864332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.52      0.60       277\n",
      "         2.0       0.48      0.68      0.56       180\n",
      "\n",
      "    accuracy                           0.58       457\n",
      "   macro avg       0.60      0.60      0.58       457\n",
      "weighted avg       0.62      0.58      0.59       457\n",
      "\n",
      "[[144 133]\n",
      " [ 57 123]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.6138328530259366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.93      0.72       185\n",
      "         2.0       0.76      0.25      0.38       162\n",
      "\n",
      "    accuracy                           0.61       347\n",
      "   macro avg       0.67      0.59      0.55       347\n",
      "weighted avg       0.67      0.61      0.56       347\n",
      "\n",
      "[[172  13]\n",
      " [121  41]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Suture_dataX_basic_120_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5061224489795918\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.58      0.65       389\n",
      "         2.0       0.12      0.23      0.16       101\n",
      "\n",
      "    accuracy                           0.51       490\n",
      "   macro avg       0.43      0.40      0.41       490\n",
      "weighted avg       0.61      0.51      0.55       490\n",
      "\n",
      "[[225 164]\n",
      " [ 78  23]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5433962264150943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.75      0.69       183\n",
      "         2.0       0.13      0.09      0.10        82\n",
      "\n",
      "    accuracy                           0.54       265\n",
      "   macro avg       0.39      0.42      0.40       265\n",
      "weighted avg       0.49      0.54      0.51       265\n",
      "\n",
      "[[137  46]\n",
      " [ 75   7]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.7280701754385965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       264\n",
      "         2.0       0.40      0.37      0.38        78\n",
      "\n",
      "    accuracy                           0.73       342\n",
      "   macro avg       0.61      0.60      0.60       342\n",
      "weighted avg       0.72      0.73      0.72       342\n",
      "\n",
      "[[220  44]\n",
      " [ 49  29]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.5144694533762058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.60      0.64       222\n",
      "         2.0       0.23      0.30      0.26        89\n",
      "\n",
      "    accuracy                           0.51       311\n",
      "   macro avg       0.46      0.45      0.45       311\n",
      "weighted avg       0.55      0.51      0.53       311\n",
      "\n",
      "[[133  89]\n",
      " [ 62  27]]\n",
      "User out:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data\n",
      "Accuracy: 0.39325842696629215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.46      0.53       199\n",
      "         2.0       0.11      0.19      0.14        68\n",
      "\n",
      "    accuracy                           0.39       267\n",
      "   macro avg       0.37      0.33      0.34       267\n",
      "weighted avg       0.49      0.39      0.43       267\n",
      "\n",
      "[[ 92 107]\n",
      " [ 55  13]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Knot_dataX_basic_120_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5174825174825175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.60      0.63        99\n",
      "         2.0       0.27      0.34      0.30        44\n",
      "\n",
      "    accuracy                           0.52       143\n",
      "   macro avg       0.47      0.47      0.47       143\n",
      "weighted avg       0.55      0.52      0.53       143\n",
      "\n",
      "[[59 40]\n",
      " [29 15]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.58      0.64        97\n",
      "         2.0       0.28      0.42      0.34        38\n",
      "\n",
      "    accuracy                           0.53       135\n",
      "   macro avg       0.50      0.50      0.49       135\n",
      "weighted avg       0.59      0.53      0.55       135\n",
      "\n",
      "[[56 41]\n",
      " [22 16]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.5443037974683544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.60      0.66       117\n",
      "         2.0       0.25      0.39      0.31        41\n",
      "\n",
      "    accuracy                           0.54       158\n",
      "   macro avg       0.50      0.49      0.48       158\n",
      "weighted avg       0.61      0.54      0.57       158\n",
      "\n",
      "[[70 47]\n",
      " [25 16]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.7222222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.79      0.81       125\n",
      "         2.0       0.41      0.49      0.44        37\n",
      "\n",
      "    accuracy                           0.72       162\n",
      "   macro avg       0.62      0.64      0.63       162\n",
      "weighted avg       0.74      0.72      0.73       162\n",
      "\n",
      "[[99 26]\n",
      " [19 18]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.6014492753623188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.59      0.68       100\n",
      "         2.0       0.37      0.63      0.47        38\n",
      "\n",
      "    accuracy                           0.60       138\n",
      "   macro avg       0.59      0.61      0.57       138\n",
      "weighted avg       0.69      0.60      0.62       138\n",
      "\n",
      "[[59 41]\n",
      " [14 24]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Needle_dataX_basic_120_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.80      0.48        40\n",
      "         2.0       0.88      0.50      0.63       121\n",
      "\n",
      "    accuracy                           0.57       161\n",
      "   macro avg       0.61      0.65      0.56       161\n",
      "weighted avg       0.75      0.57      0.60       161\n",
      "\n",
      "[[32  8]\n",
      " [61 60]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.68      0.66       145\n",
      "         2.0       0.02      0.02      0.02        53\n",
      "\n",
      "    accuracy                           0.50       198\n",
      "   macro avg       0.34      0.35      0.34       198\n",
      "weighted avg       0.48      0.50      0.49       198\n",
      "\n",
      "[[98 47]\n",
      " [52  1]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.3783783783783784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.57      0.49       118\n",
      "         2.0       0.25      0.16      0.20       104\n",
      "\n",
      "    accuracy                           0.38       222\n",
      "   macro avg       0.34      0.37      0.35       222\n",
      "weighted avg       0.35      0.38      0.35       222\n",
      "\n",
      "[[67 51]\n",
      " [87 17]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.39737991266375544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.46      0.48       139\n",
      "         2.0       0.26      0.30      0.28        90\n",
      "\n",
      "    accuracy                           0.40       229\n",
      "   macro avg       0.38      0.38      0.38       229\n",
      "weighted avg       0.41      0.40      0.40       229\n",
      "\n",
      "[[64 75]\n",
      " [63 27]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5942857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.94      0.71        93\n",
      "         2.0       0.74      0.21      0.32        82\n",
      "\n",
      "    accuracy                           0.59       175\n",
      "   macro avg       0.66      0.57      0.52       175\n",
      "weighted avg       0.65      0.59      0.53       175\n",
      "\n",
      "[[87  6]\n",
      " [65 17]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Suture_dataX_basic_150_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5097636176772867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.61      0.66       773\n",
      "         2.0       0.08      0.12      0.09       200\n",
      "\n",
      "    accuracy                           0.51       973\n",
      "   macro avg       0.40      0.37      0.38       973\n",
      "weighted avg       0.59      0.51      0.55       973\n",
      "\n",
      "[[471 302]\n",
      " [175  25]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.6500956022944551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.91      0.78       362\n",
      "         2.0       0.26      0.07      0.12       161\n",
      "\n",
      "    accuracy                           0.65       523\n",
      "   macro avg       0.47      0.49      0.45       523\n",
      "weighted avg       0.56      0.65      0.58       523\n",
      "\n",
      "[[328  34]\n",
      " [149  12]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.6780415430267063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.80      0.79       522\n",
      "         2.0       0.28      0.27      0.27       152\n",
      "\n",
      "    accuracy                           0.68       674\n",
      "   macro avg       0.53      0.53      0.53       674\n",
      "weighted avg       0.67      0.68      0.68       674\n",
      "\n",
      "[[416 106]\n",
      " [111  41]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6368078175895765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.69      0.73       438\n",
      "         2.0       0.40      0.51      0.45       176\n",
      "\n",
      "    accuracy                           0.64       614\n",
      "   macro avg       0.59      0.60      0.59       614\n",
      "weighted avg       0.67      0.64      0.65       614\n",
      "\n",
      "[[301 137]\n",
      " [ 86  90]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.3231939163498099\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.37      0.45       393\n",
      "         2.0       0.09      0.17      0.11       133\n",
      "\n",
      "    accuracy                           0.32       526\n",
      "   macro avg       0.33      0.27      0.28       526\n",
      "weighted avg       0.45      0.32      0.37       526\n",
      "\n",
      "[[147 246]\n",
      " [110  23]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Knot_dataX_basic_150_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.7813620071684588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.74      0.82       194\n",
      "         2.0       0.60      0.88      0.71        85\n",
      "\n",
      "    accuracy                           0.78       279\n",
      "   macro avg       0.76      0.81      0.77       279\n",
      "weighted avg       0.83      0.78      0.79       279\n",
      "\n",
      "[[143  51]\n",
      " [ 10  75]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.7099236641221374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.76      0.79       189\n",
      "         2.0       0.48      0.59      0.53        73\n",
      "\n",
      "    accuracy                           0.71       262\n",
      "   macro avg       0.65      0.67      0.66       262\n",
      "weighted avg       0.73      0.71      0.72       262\n",
      "\n",
      "[[143  46]\n",
      " [ 30  43]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.7091503267973857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.81      0.81       227\n",
      "         2.0       0.43      0.42      0.43        79\n",
      "\n",
      "    accuracy                           0.71       306\n",
      "   macro avg       0.62      0.61      0.62       306\n",
      "weighted avg       0.71      0.71      0.71       306\n",
      "\n",
      "[[184  43]\n",
      " [ 46  33]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6253968253968254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.70      0.74       245\n",
      "         2.0       0.26      0.37      0.31        70\n",
      "\n",
      "    accuracy                           0.63       315\n",
      "   macro avg       0.53      0.53      0.52       315\n",
      "weighted avg       0.68      0.63      0.65       315\n",
      "\n",
      "[[171  74]\n",
      " [ 44  26]]\n",
      "User out:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data\n",
      "Accuracy: 0.6245353159851301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.73      0.74       196\n",
      "         2.0       0.32      0.33      0.32        73\n",
      "\n",
      "    accuracy                           0.62       269\n",
      "   macro avg       0.53      0.53      0.53       269\n",
      "weighted avg       0.63      0.62      0.63       269\n",
      "\n",
      "[[144  52]\n",
      " [ 49  24]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Needle_dataX_basic_150_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5283018867924528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.94      0.49        78\n",
      "         2.0       0.95      0.40      0.56       240\n",
      "\n",
      "    accuracy                           0.53       318\n",
      "   macro avg       0.64      0.67      0.53       318\n",
      "weighted avg       0.80      0.53      0.54       318\n",
      "\n",
      "[[ 73   5]\n",
      " [145  95]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5256410256410257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.48      0.60       286\n",
      "         2.0       0.31      0.64      0.42       104\n",
      "\n",
      "    accuracy                           0.53       390\n",
      "   macro avg       0.55      0.56      0.51       390\n",
      "weighted avg       0.66      0.53      0.55       390\n",
      "\n",
      "[[138 148]\n",
      " [ 37  67]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.4383561643835616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.66      0.55       233\n",
      "         2.0       0.33      0.19      0.24       205\n",
      "\n",
      "    accuracy                           0.44       438\n",
      "   macro avg       0.40      0.42      0.40       438\n",
      "weighted avg       0.41      0.44      0.41       438\n",
      "\n",
      "[[153  80]\n",
      " [166  39]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6150442477876106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.51      0.62       274\n",
      "         2.0       0.51      0.77      0.61       178\n",
      "\n",
      "    accuracy                           0.62       452\n",
      "   macro avg       0.64      0.64      0.62       452\n",
      "weighted avg       0.67      0.62      0.62       452\n",
      "\n",
      "[[141 133]\n",
      " [ 41 137]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5510204081632653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.91      0.68       183\n",
      "         2.0       0.58      0.14      0.22       160\n",
      "\n",
      "    accuracy                           0.55       343\n",
      "   macro avg       0.56      0.53      0.45       343\n",
      "weighted avg       0.56      0.55      0.47       343\n",
      "\n",
      "[[167  16]\n",
      " [138  22]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Suture_dataX_basic_150_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5685071574642127\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.67      0.71       388\n",
      "         2.0       0.12      0.18      0.15       101\n",
      "\n",
      "    accuracy                           0.57       489\n",
      "   macro avg       0.44      0.42      0.43       489\n",
      "weighted avg       0.63      0.57      0.59       489\n",
      "\n",
      "[[260 128]\n",
      " [ 83  18]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.6539923954372624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.86      0.78       182\n",
      "         2.0       0.38      0.19      0.25        81\n",
      "\n",
      "    accuracy                           0.65       263\n",
      "   macro avg       0.54      0.52      0.51       263\n",
      "weighted avg       0.60      0.65      0.61       263\n",
      "\n",
      "[[157  25]\n",
      " [ 66  15]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.7307692307692307\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       262\n",
      "         2.0       0.40      0.41      0.41        76\n",
      "\n",
      "    accuracy                           0.73       338\n",
      "   macro avg       0.62      0.62      0.62       338\n",
      "weighted avg       0.73      0.73      0.73       338\n",
      "\n",
      "[[216  46]\n",
      " [ 45  31]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6343042071197411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.67      0.72       220\n",
      "         2.0       0.40      0.54      0.46        89\n",
      "\n",
      "    accuracy                           0.63       309\n",
      "   macro avg       0.59      0.61      0.59       309\n",
      "weighted avg       0.67      0.63      0.65       309\n",
      "\n",
      "[[148  72]\n",
      " [ 41  48]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.30943396226415093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.37      0.44       198\n",
      "         2.0       0.07      0.13      0.09        67\n",
      "\n",
      "    accuracy                           0.31       265\n",
      "   macro avg       0.31      0.25      0.27       265\n",
      "weighted avg       0.43      0.31      0.35       265\n",
      "\n",
      "[[ 73 125]\n",
      " [ 58   9]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Knot_dataX_basic_150_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.6312056737588653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.59      0.69        98\n",
      "         2.0       0.44      0.72      0.54        43\n",
      "\n",
      "    accuracy                           0.63       141\n",
      "   macro avg       0.63      0.66      0.62       141\n",
      "weighted avg       0.71      0.63      0.65       141\n",
      "\n",
      "[[58 40]\n",
      " [12 31]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.6893939393939394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.74      0.77        95\n",
      "         2.0       0.46      0.57      0.51        37\n",
      "\n",
      "    accuracy                           0.69       132\n",
      "   macro avg       0.64      0.65      0.64       132\n",
      "weighted avg       0.71      0.69      0.70       132\n",
      "\n",
      "[[70 25]\n",
      " [16 21]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.6233766233766234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.68      0.73       114\n",
      "         2.0       0.33      0.45      0.38        40\n",
      "\n",
      "    accuracy                           0.62       154\n",
      "   macro avg       0.56      0.57      0.56       154\n",
      "weighted avg       0.66      0.62      0.64       154\n",
      "\n",
      "[[78 36]\n",
      " [22 18]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.5822784810126582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.62      0.70       123\n",
      "         2.0       0.25      0.46      0.33        35\n",
      "\n",
      "    accuracy                           0.58       158\n",
      "   macro avg       0.53      0.54      0.51       158\n",
      "weighted avg       0.68      0.58      0.62       158\n",
      "\n",
      "[[76 47]\n",
      " [19 16]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.6176470588235294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.72      0.73        99\n",
      "         2.0       0.32      0.35      0.33        37\n",
      "\n",
      "    accuracy                           0.62       136\n",
      "   macro avg       0.53      0.53      0.53       136\n",
      "weighted avg       0.63      0.62      0.62       136\n",
      "\n",
      "[[71 28]\n",
      " [24 13]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Needle_dataX_basic_150_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.6875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.95      0.60        39\n",
      "         2.0       0.97      0.60      0.74       121\n",
      "\n",
      "    accuracy                           0.69       160\n",
      "   macro avg       0.70      0.78      0.67       160\n",
      "weighted avg       0.84      0.69      0.71       160\n",
      "\n",
      "[[37  2]\n",
      " [48 73]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.41836734693877553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.51      0.56       144\n",
      "         2.0       0.11      0.17      0.14        52\n",
      "\n",
      "    accuracy                           0.42       196\n",
      "   macro avg       0.37      0.34      0.35       196\n",
      "weighted avg       0.49      0.42      0.45       196\n",
      "\n",
      "[[73 71]\n",
      " [43  9]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.4818181818181818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.67      0.58       117\n",
      "         2.0       0.42      0.27      0.33       103\n",
      "\n",
      "    accuracy                           0.48       220\n",
      "   macro avg       0.46      0.47      0.45       220\n",
      "weighted avg       0.47      0.48      0.46       220\n",
      "\n",
      "[[78 39]\n",
      " [75 28]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6096491228070176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.46      0.59       138\n",
      "         2.0       0.50      0.84      0.63        90\n",
      "\n",
      "    accuracy                           0.61       228\n",
      "   macro avg       0.66      0.65      0.61       228\n",
      "weighted avg       0.69      0.61      0.60       228\n",
      "\n",
      "[[63 75]\n",
      " [14 76]]\n",
      "User out:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data\n",
      "Accuracy: 0.5232558139534884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.87      0.66        92\n",
      "         2.0       0.45      0.12      0.20        80\n",
      "\n",
      "    accuracy                           0.52       172\n",
      "   macro avg       0.49      0.50      0.43       172\n",
      "weighted avg       0.50      0.52      0.44       172\n",
      "\n",
      "[[80 12]\n",
      " [70 10]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Suture_dataX_basic_180_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5015511892450879\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.59      0.65       769\n",
      "         2.0       0.09      0.16      0.12       198\n",
      "\n",
      "    accuracy                           0.50       967\n",
      "   macro avg       0.41      0.38      0.38       967\n",
      "weighted avg       0.60      0.50      0.54       967\n",
      "\n",
      "[[453 316]\n",
      " [166  32]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.6428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.90      0.78       359\n",
      "         2.0       0.23      0.07      0.11       159\n",
      "\n",
      "    accuracy                           0.64       518\n",
      "   macro avg       0.46      0.48      0.44       518\n",
      "weighted avg       0.55      0.64      0.57       518\n",
      "\n",
      "[[322  37]\n",
      " [148  11]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.6811377245508982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.80      0.79       518\n",
      "         2.0       0.29      0.29      0.29       150\n",
      "\n",
      "    accuracy                           0.68       668\n",
      "   macro avg       0.54      0.54      0.54       668\n",
      "weighted avg       0.68      0.68      0.68       668\n",
      "\n",
      "[[412 106]\n",
      " [107  43]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6019736842105263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.64      0.70       434\n",
      "         2.0       0.36      0.51      0.42       174\n",
      "\n",
      "    accuracy                           0.60       608\n",
      "   macro avg       0.56      0.57      0.56       608\n",
      "weighted avg       0.65      0.60      0.62       608\n",
      "\n",
      "[[278 156]\n",
      " [ 86  88]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.29615384615384616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.36      0.44       389\n",
      "         2.0       0.05      0.10      0.07       131\n",
      "\n",
      "    accuracy                           0.30       520\n",
      "   macro avg       0.30      0.23      0.25       520\n",
      "weighted avg       0.42      0.30      0.34       520\n",
      "\n",
      "[[141 248]\n",
      " [118  13]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Knot_dataX_basic_180_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.7846715328467153\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.72      0.82       191\n",
      "         2.0       0.59      0.93      0.72        83\n",
      "\n",
      "    accuracy                           0.78       274\n",
      "   macro avg       0.78      0.83      0.77       274\n",
      "weighted avg       0.85      0.78      0.79       274\n",
      "\n",
      "[[138  53]\n",
      " [  6  77]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.7821011673151751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.88      0.85       186\n",
      "         2.0       0.63      0.52      0.57        71\n",
      "\n",
      "    accuracy                           0.78       257\n",
      "   macro avg       0.73      0.70      0.71       257\n",
      "weighted avg       0.77      0.78      0.78       257\n",
      "\n",
      "[[164  22]\n",
      " [ 34  37]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.77\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.86      0.85       223\n",
      "         2.0       0.56      0.51      0.53        77\n",
      "\n",
      "    accuracy                           0.77       300\n",
      "   macro avg       0.70      0.68      0.69       300\n",
      "weighted avg       0.76      0.77      0.77       300\n",
      "\n",
      "[[192  31]\n",
      " [ 38  39]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.7258064516129032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.78      0.82       242\n",
      "         2.0       0.40      0.53      0.46        68\n",
      "\n",
      "    accuracy                           0.73       310\n",
      "   macro avg       0.63      0.66      0.64       310\n",
      "weighted avg       0.76      0.73      0.74       310\n",
      "\n",
      "[[189  53]\n",
      " [ 32  36]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.6287878787878788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.76      0.75       193\n",
      "         2.0       0.30      0.28      0.29        71\n",
      "\n",
      "    accuracy                           0.63       264\n",
      "   macro avg       0.52      0.52      0.52       264\n",
      "weighted avg       0.62      0.63      0.63       264\n",
      "\n",
      "[[146  47]\n",
      " [ 51  20]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Needle_dataX_basic_180_30.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.38095238095238093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.94      0.42        77\n",
      "         2.0       0.91      0.20      0.33       238\n",
      "\n",
      "    accuracy                           0.38       315\n",
      "   macro avg       0.59      0.57      0.38       315\n",
      "weighted avg       0.75      0.38      0.35       315\n",
      "\n",
      "[[ 72   5]\n",
      " [190  48]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.5440414507772021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.65      0.68       283\n",
      "         2.0       0.21      0.26      0.23       103\n",
      "\n",
      "    accuracy                           0.54       386\n",
      "   macro avg       0.46      0.45      0.46       386\n",
      "weighted avg       0.57      0.54      0.56       386\n",
      "\n",
      "[[183 100]\n",
      " [ 76  27]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.46543778801843316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.60      0.55       231\n",
      "         2.0       0.41      0.31      0.35       203\n",
      "\n",
      "    accuracy                           0.47       434\n",
      "   macro avg       0.45      0.46      0.45       434\n",
      "weighted avg       0.46      0.47      0.45       434\n",
      "\n",
      "[[139  92]\n",
      " [140  63]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.46308724832214765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.49      0.53       271\n",
      "         2.0       0.35      0.41      0.38       176\n",
      "\n",
      "    accuracy                           0.46       447\n",
      "   macro avg       0.46      0.45      0.45       447\n",
      "weighted avg       0.48      0.46      0.47       447\n",
      "\n",
      "[[134 137]\n",
      " [103  73]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5634218289085545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.88      0.68       181\n",
      "         2.0       0.59      0.20      0.30       158\n",
      "\n",
      "    accuracy                           0.56       339\n",
      "   macro avg       0.58      0.54      0.49       339\n",
      "weighted avg       0.57      0.56      0.51       339\n",
      "\n",
      "[[159  22]\n",
      " [126  32]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Suture_dataX_basic_180_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.4524793388429752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.52      0.60       385\n",
      "         2.0       0.09      0.17      0.11        99\n",
      "\n",
      "    accuracy                           0.45       484\n",
      "   macro avg       0.40      0.35      0.36       484\n",
      "weighted avg       0.58      0.45      0.50       484\n",
      "\n",
      "[[202 183]\n",
      " [ 82  17]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.6923076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.92      0.80       180\n",
      "         2.0       0.50      0.19      0.27        80\n",
      "\n",
      "    accuracy                           0.69       260\n",
      "   macro avg       0.61      0.55      0.54       260\n",
      "weighted avg       0.65      0.69      0.64       260\n",
      "\n",
      "[[165  15]\n",
      " [ 65  15]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.7202380952380952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.82       260\n",
      "         2.0       0.40      0.45      0.42        76\n",
      "\n",
      "    accuracy                           0.72       336\n",
      "   macro avg       0.61      0.62      0.62       336\n",
      "weighted avg       0.73      0.72      0.73       336\n",
      "\n",
      "[[208  52]\n",
      " [ 42  34]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.5540983606557377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.60      0.66       218\n",
      "         2.0       0.30      0.44      0.36        87\n",
      "\n",
      "    accuracy                           0.55       305\n",
      "   macro avg       0.52      0.52      0.51       305\n",
      "weighted avg       0.61      0.55      0.57       305\n",
      "\n",
      "[[131  87]\n",
      " [ 49  38]]\n",
      "User out:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data\n",
      "Accuracy: 0.24521072796934865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.28      0.36       195\n",
      "         2.0       0.06      0.14      0.08        66\n",
      "\n",
      "    accuracy                           0.25       261\n",
      "   macro avg       0.28      0.21      0.22       261\n",
      "weighted avg       0.38      0.25      0.29       261\n",
      "\n",
      "[[ 55 140]\n",
      " [ 57   9]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Knot_dataX_basic_180_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.6594202898550725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.67      0.73        96\n",
      "         2.0       0.46      0.64      0.53        42\n",
      "\n",
      "    accuracy                           0.66       138\n",
      "   macro avg       0.63      0.65      0.63       138\n",
      "weighted avg       0.70      0.66      0.67       138\n",
      "\n",
      "[[64 32]\n",
      " [15 27]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.77      0.79        94\n",
      "         2.0       0.46      0.53      0.49        36\n",
      "\n",
      "    accuracy                           0.70       130\n",
      "   macro avg       0.64      0.65      0.64       130\n",
      "weighted avg       0.71      0.70      0.71       130\n",
      "\n",
      "[[72 22]\n",
      " [17 19]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.6644736842105263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.74      0.77       113\n",
      "         2.0       0.37      0.44      0.40        39\n",
      "\n",
      "    accuracy                           0.66       152\n",
      "   macro avg       0.58      0.59      0.58       152\n",
      "weighted avg       0.68      0.66      0.67       152\n",
      "\n",
      "[[84 29]\n",
      " [22 17]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6751592356687898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.74      0.78       122\n",
      "         2.0       0.33      0.46      0.39        35\n",
      "\n",
      "    accuracy                           0.68       157\n",
      "   macro avg       0.58      0.60      0.58       157\n",
      "weighted avg       0.72      0.68      0.69       157\n",
      "\n",
      "[[90 32]\n",
      " [19 16]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.6842105263157895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.80      0.79        97\n",
      "         2.0       0.41      0.36      0.38        36\n",
      "\n",
      "    accuracy                           0.68       133\n",
      "   macro avg       0.59      0.58      0.59       133\n",
      "weighted avg       0.67      0.68      0.68       133\n",
      "\n",
      "[[78 19]\n",
      " [23 13]]\n",
      "loading the dataset\n",
      "../data/augment_data_from_csvs/no_interm/Needle_dataX_basic_180_60.mat  loaded\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.4430379746835443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.30      0.92      0.45        39\n",
      "         2.0       0.92      0.29      0.44       119\n",
      "\n",
      "    accuracy                           0.44       158\n",
      "   macro avg       0.61      0.60      0.44       158\n",
      "weighted avg       0.77      0.44      0.44       158\n",
      "\n",
      "[[36  3]\n",
      " [85 34]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.4072164948453608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.48      0.54       142\n",
      "         2.0       0.13      0.21      0.16        52\n",
      "\n",
      "    accuracy                           0.41       194\n",
      "   macro avg       0.38      0.35      0.35       194\n",
      "weighted avg       0.49      0.41      0.44       194\n",
      "\n",
      "[[68 74]\n",
      " [41 11]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.4036697247706422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.50      0.47       116\n",
      "         2.0       0.34      0.29      0.32       102\n",
      "\n",
      "    accuracy                           0.40       218\n",
      "   macro avg       0.39      0.40      0.39       218\n",
      "weighted avg       0.40      0.40      0.40       218\n",
      "\n",
      "[[58 58]\n",
      " [72 30]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.41964285714285715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.45      0.48       136\n",
      "         2.0       0.31      0.38      0.34        88\n",
      "\n",
      "    accuracy                           0.42       224\n",
      "   macro avg       0.42      0.41      0.41       224\n",
      "weighted avg       0.44      0.42      0.43       224\n",
      "\n",
      "[[61 75]\n",
      " [55 33]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5614035087719298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.91      0.69        91\n",
      "         2.0       0.62      0.16      0.26        80\n",
      "\n",
      "    accuracy                           0.56       171\n",
      "   macro avg       0.59      0.54      0.47       171\n",
      "weighted avg       0.58      0.56      0.49       171\n",
      "\n",
      "[[83  8]\n",
      " [67 13]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr 17 17:00:02 2019\n",
    "\n",
    "@author: xngu0004\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "list_skill = [\"Suture\", \"Knot\", \"Needle\"]\n",
    "list_win = [90, 120, 150, 180]\n",
    "list_over = [30, 60]\n",
    "list_netw = [1]\n",
    "\n",
    "# users in Group\n",
    "novice = ['B', 'G', 'H', 'I']\n",
    "intermediate = ['C', 'F']\n",
    "expert = ['D', 'E']\n",
    "\n",
    "f_s = 30 # Sampling rate\n",
    "denominator = 10\n",
    "\n",
    "# Function to calculate FFT coefficiences\n",
    "def get_fft_values(y_values, T, N, f_s):\n",
    "    f_values = np.linspace(0.0, 1.0/(T), N)\n",
    "    fft_values_ = fft(y_values)\n",
    "    fft_values = 1.0/N * np.abs(fft_values_[0:N//2])\n",
    "    return f_values, fft_values\n",
    "\n",
    "# Revised from http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n",
    "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising', kpsh=False, valley=False, ax=None):\n",
    "\n",
    "    x = np.atleast_1d(x).astype('float64')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    # handle NaN's\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    # handle NaN's\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size-1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                    & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    return ind\n",
    "\n",
    "# Extract no_peaks\n",
    "def get_first_n_peaks(x,y,no_peaks=10):\n",
    "    x_, y_ = list(x), list(y)\n",
    "    if len(x_) >= no_peaks:\n",
    "        return x_[:no_peaks], y_[:no_peaks]\n",
    "    else:\n",
    "        missing_no_peaks = no_peaks-len(x_)\n",
    "        return x_ + [0]*missing_no_peaks, y_ + [0]*missing_no_peaks\n",
    "    \n",
    "def get_features(x_values, y_values, mph):\n",
    "    indices_peaks = detect_peaks(y_values, mph=mph)\n",
    "    peaks_x, peaks_y = get_first_n_peaks(x_values[indices_peaks], y_values[indices_peaks])\n",
    "    return peaks_x + peaks_y\n",
    "\n",
    "# Extract features\n",
    "def extract_features_labels(dataset, labels, T, N, f_s, denominator):\n",
    "    percentile = 5\n",
    "    list_of_features = []\n",
    "    list_of_labels = []\n",
    "    for signal_no in range(0, len(dataset)):\n",
    "        features = []\n",
    "        list_of_labels.append(labels[signal_no])\n",
    "        for signal_comp in range(0,dataset.shape[2]):\n",
    "            signal = dataset[signal_no, :, signal_comp]\n",
    "            \n",
    "            signal_min = np.nanpercentile(signal, percentile)\n",
    "            signal_max = np.nanpercentile(signal, 100-percentile)\n",
    "            ##ijk = (100 - 2*percentile)/10\n",
    "            mph = signal_min + (signal_max - signal_min)/denominator\n",
    "            \n",
    "            #features += get_features(*get_psd_values(signal, T, N, f_s), mph)\n",
    "            features += get_features(*get_fft_values(signal, T, N, f_s), mph)\n",
    "            #features += get_features(*get_autocorr_values(signal, T, N, f_s), mph)\n",
    "        list_of_features.append(features)\n",
    "    return np.array(list_of_features), np.array(list_of_labels)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "for win in list_win:\n",
    "    for over in list_over:\n",
    "        f_s = over #(it is set to 30 by default, but I noticed that maybe it needs to switch to 60 when over is 60)\n",
    "        for skill in list_skill:\n",
    "            input_path = input_root+skill+'_dataX_basic_'+str(win)+'_'+str(over)+'.mat'\n",
    "            \" Load the data\"\n",
    "            print('loading the dataset')\n",
    "            dataset = sio.loadmat(input_path)\n",
    "            print(input_path, ' loaded')\n",
    "            features = dataset['dataX']\n",
    "            sizeD_ts = (((features[0,0:1])[0])[0,:])[0].shape[0]\n",
    "            sizeD_va = (((features[0,0:1])[0])[0,:])[0].shape[1]\n",
    "            N = sizeD_ts # Number of samples\n",
    "            t_n = N/f_s # Duration of window\n",
    "            T = 1 / f_s # Period\n",
    "            print('loaded the dataset')\n",
    "            for netw in list_netw:\n",
    "                for tim in range(3,4):\n",
    "                    full_output_path = output_root + 'DFT/' + str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+'_'+str(netw)+'_report_DFT_fea.txt'\n",
    "                    if generateOnlyMissingResultFiles and path.exists(full_output_path):\n",
    "                        print(full_output_path + ' already exists')\n",
    "                        continue\n",
    "                    acc_1 = []\n",
    "                    accu = 0\n",
    "                    acc_1_fea = []\n",
    "                    accu_fea = 0                    \n",
    "                    ###################################\n",
    "                    #Pre-process data\n",
    "                    for user in range(1,6):\n",
    "                        \" Initialize data for concatenation \"\n",
    "                        X_test1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "                        y_test1 = np.ones(1)\n",
    "                        X_train1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "                        y_train1 = np.ones(1)\n",
    "                        print(\"User out: \", user)\n",
    "                        # Extract data for training and testing\n",
    "                        for j in groupRange:\n",
    "                            upper_range =  (((features[0,j:j+1])[0])[0,:]).shape[0]\n",
    "                            for i in range(0, upper_range):\n",
    "                                k = (((features[0,j:j+1])[0])[2,:])[i]\n",
    "                                if (k == user): # take out the ith trial of each user for testing\n",
    "                                    temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                                    temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                                    X_test1 = np.concatenate((X_test1, temp_x1), axis=0) \n",
    "                                    user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                                    if(user_temp in novice):\n",
    "                                        y_test1 = np.concatenate((y_test1,[0]))\n",
    "                                    elif(user_temp in intermediate):\n",
    "                                        y_test1 = np.concatenate((y_test1,[1]))\n",
    "                                    elif(user_temp in expert):\n",
    "                                        y_test1 = np.concatenate((y_test1,[2]))\n",
    "                                else:\n",
    "                                    temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                                    temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                                    X_train1 = np.concatenate((X_train1, temp_x1), axis=0) \n",
    "                                    user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                                    if(user_temp in novice):\n",
    "                                        y_train1 = np.concatenate((y_train1,[0]))\n",
    "                                    elif(user_temp in intermediate):\n",
    "                                        y_train1 = np.concatenate((y_train1,[1]))\n",
    "                                    elif(user_temp in expert):\n",
    "                                        y_train1 = np.concatenate((y_train1,[2]))\n",
    "        \n",
    "                        \" Delete the first column \"\n",
    "                        X_test1 = np.delete(X_test1,np.s_[0:1], axis=0)\n",
    "                        y_test1 = np.delete(y_test1,np.s_[0:1])\n",
    "                        X_train1 = np.delete(X_train1,np.s_[0:1], axis=0)\n",
    "                        y_train1= np.delete(y_train1,np.s_[0:1])\n",
    "                        \n",
    "                        print(\"Shuffle data\")\n",
    "                        \" Shuffle the training and testing data \"\n",
    "                        Xx_test, yy_test = shuffle(X_test1, y_test1, random_state = random.randint(10,50))\n",
    "                        Xx_train, yy_train = shuffle(X_train1, y_train1, random_state = random.randint(10,50))\n",
    "            \n",
    "                        ##############################################\n",
    "                        X_train, Y_train = extract_features_labels(Xx_train, yy_train, T, N, f_s, denominator)\n",
    "                        X_test, Y_test = extract_features_labels(Xx_test, yy_test, T, N, f_s, denominator)\n",
    "                        ##############################################\n",
    "                        \n",
    "                        svm1 = svm.SVC(kernel = 'linear')\n",
    "                        svm1.fit(X_train, Y_train)\n",
    "                        prediction_fea = svm1.predict(X_test)\n",
    "                        y_pred_fea = prediction_fea\n",
    "                        cr_fea = classification_report(Y_test, y_pred_fea)\n",
    "                        cm_fea = confusion_matrix(Y_test, y_pred_fea)\n",
    "    \n",
    "                        acc_fea = np.sum(y_pred_fea == Y_test)/y_pred_fea.shape[0]\n",
    "                        acc_1_fea.append(acc_fea)\n",
    "                        print('Accuracy: ' + str(acc_fea))\n",
    "                        print(cr_fea)\n",
    "                        print(cm_fea)\n",
    "                        f = open(full_output_path, 'a+')\n",
    "                        f.write(\"-------------------- DFT Features -----------------------\")\n",
    "                        f.write('---------' + skill + \" Win: \" + str(win) + \" Over: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "                        f.write('Accuracy: ' + str(acc_fea))\n",
    "                        f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr_fea, cm_fea))\n",
    "                    for i in range(0,5):\n",
    "                        accu_fea += acc_1_fea[i]\n",
    "                    accu_fea = accu_fea/5\n",
    "                    f.write(\"\\n-------------------------------------------\\n\")\n",
    "                    f.write(\"\\nAcc_features = \" + str(accu_fea))\n",
    "                    f.write(\"\\n-------------------------------------------\\n\")\n",
    "                    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate user data will be skipped\n",
      "[0, 2]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['../data/augment_data_from_csvs/',\n",
    "          '../data/augment_data_from_csvs/no_interm/',\n",
    "]\n",
    "\n",
    "outputs = ['../results/csv_based/',\n",
    "           '../results/csv_without_interms/',\n",
    "]\n",
    "\n",
    "ind = 1\n",
    "\n",
    "withoutInterms = (ind == 1)\n",
    "\n",
    "output_root = outputs[ind]\n",
    "input_root = inputs[ind]\n",
    "\n",
    "if withoutInterms:\n",
    "    print('Intermediate user data will be skipped')\n",
    "else:\n",
    "    print('All three user groups (Beginner, Intermediate, Expert) are processed')\n",
    "    \n",
    "groupRange = [0,2]\n",
    "if not withoutInterms:\n",
    "    groupRange = [0,1,2]\n",
    "print(groupRange)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "generateOnlyMissingResultFiles = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Suture_90_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Suture_90_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Knot_90_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Knot_90_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Needle_90_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Needle_90_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Suture_90_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Suture_90_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Knot_90_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Knot_90_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Needle_90_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Needle_90_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Suture_120_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Suture_120_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Knot_120_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Knot_120_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Needle_120_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Needle_120_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Suture_120_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Suture_120_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Knot_120_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Knot_120_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Needle_120_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Needle_120_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Suture_150_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Suture_150_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Knot_150_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Knot_150_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Needle_150_30_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Needle_150_30_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Suture_150_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Suture_150_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Knot_150_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Knot_150_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_without_interms/DCT/3_Needle_150_60_1_report_DCT_fea.txt already exists\n",
      "../results/csv_without_interms/DCT/4_Needle_150_60_1_report_DCT_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5412322274881517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.62      0.67       769\n",
      "         1.0       0.37      0.61      0.46        88\n",
      "         2.0       0.16      0.21      0.18       198\n",
      "\n",
      "    accuracy                           0.54      1055\n",
      "   macro avg       0.42      0.48      0.44      1055\n",
      "weighted avg       0.60      0.54      0.56      1055\n",
      "\n",
      "[[475  85 209]\n",
      " [ 25  54   9]\n",
      " [148   8  42]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.4527027027027027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.64      0.62       359\n",
      "         1.0       0.02      0.03      0.03        74\n",
      "         2.0       0.29      0.22      0.25       159\n",
      "\n",
      "    accuracy                           0.45       592\n",
      "   macro avg       0.30      0.30      0.30       592\n",
      "weighted avg       0.44      0.45      0.44       592\n",
      "\n",
      "[[231  79  49]\n",
      " [ 37   2  35]\n",
      " [122   2  35]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.5609756097560976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.68      0.73       518\n",
      "         1.0       0.11      0.26      0.16        70\n",
      "         2.0       0.32      0.31      0.32       150\n",
      "\n",
      "    accuracy                           0.56       738\n",
      "   macro avg       0.41      0.41      0.40       738\n",
      "weighted avg       0.64      0.56      0.59       738\n",
      "\n",
      "[[350  97  71]\n",
      " [ 27  18  25]\n",
      " [ 62  42  46]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6700879765395894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.83      0.81       434\n",
      "         1.0       0.46      0.53      0.49        74\n",
      "         2.0       0.42      0.33      0.37       174\n",
      "\n",
      "    accuracy                           0.67       682\n",
      "   macro avg       0.56      0.56      0.56       682\n",
      "weighted avg       0.66      0.67      0.66       682\n",
      "\n",
      "[[360   7  67]\n",
      " [ 21  39  14]\n",
      " [ 78  38  58]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.2646048109965636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.35      0.41       389\n",
      "         1.0       0.20      0.16      0.18        62\n",
      "         2.0       0.03      0.06      0.04       131\n",
      "\n",
      "    accuracy                           0.26       582\n",
      "   macro avg       0.24      0.19      0.21       582\n",
      "weighted avg       0.35      0.26      0.30       582\n",
      "\n",
      "[[136  32 221]\n",
      " [ 30  10  22]\n",
      " [115   8   8]]\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.5412322274881517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.62      0.67       769\n",
      "         1.0       0.37      0.61      0.46        88\n",
      "         2.0       0.16      0.21      0.18       198\n",
      "\n",
      "    accuracy                           0.54      1055\n",
      "   macro avg       0.42      0.48      0.44      1055\n",
      "weighted avg       0.60      0.54      0.56      1055\n",
      "\n",
      "[[475  85 209]\n",
      " [ 25  54   9]\n",
      " [148   8  42]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.4527027027027027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.64      0.62       359\n",
      "         1.0       0.02      0.03      0.03        74\n",
      "         2.0       0.29      0.22      0.25       159\n",
      "\n",
      "    accuracy                           0.45       592\n",
      "   macro avg       0.30      0.30      0.30       592\n",
      "weighted avg       0.44      0.45      0.44       592\n",
      "\n",
      "[[231  79  49]\n",
      " [ 37   2  35]\n",
      " [122   2  35]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.5609756097560976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.68      0.73       518\n",
      "         1.0       0.11      0.26      0.16        70\n",
      "         2.0       0.32      0.31      0.32       150\n",
      "\n",
      "    accuracy                           0.56       738\n",
      "   macro avg       0.41      0.41      0.40       738\n",
      "weighted avg       0.64      0.56      0.59       738\n",
      "\n",
      "[[350  97  71]\n",
      " [ 27  18  25]\n",
      " [ 62  42  46]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.6700879765395894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.83      0.81       434\n",
      "         1.0       0.46      0.53      0.49        74\n",
      "         2.0       0.42      0.33      0.37       174\n",
      "\n",
      "    accuracy                           0.67       682\n",
      "   macro avg       0.56      0.56      0.56       682\n",
      "weighted avg       0.66      0.67      0.66       682\n",
      "\n",
      "[[360   7  67]\n",
      " [ 21  39  14]\n",
      " [ 78  38  58]]\n",
      "User out:  5\n",
      "Shuffle data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2646048109965636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.35      0.41       389\n",
      "         1.0       0.20      0.16      0.18        62\n",
      "         2.0       0.03      0.06      0.04       131\n",
      "\n",
      "    accuracy                           0.26       582\n",
      "   macro avg       0.24      0.19      0.21       582\n",
      "weighted avg       0.35      0.26      0.30       582\n",
      "\n",
      "[[136  32 221]\n",
      " [ 30  10  22]\n",
      " [115   8   8]]\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.782608695652174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.82      0.86       191\n",
      "         1.0       0.33      0.04      0.07        25\n",
      "         2.0       0.62      0.92      0.74        83\n",
      "\n",
      "    accuracy                           0.78       299\n",
      "   macro avg       0.62      0.59      0.56       299\n",
      "weighted avg       0.78      0.78      0.76       299\n",
      "\n",
      "[[157   2  32]\n",
      " [ 10   1  14]\n",
      " [  7   0  76]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.712280701754386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.77      0.80       186\n",
      "         1.0       0.43      0.57      0.49        28\n",
      "         2.0       0.57      0.61      0.59        71\n",
      "\n",
      "    accuracy                           0.71       285\n",
      "   macro avg       0.61      0.65      0.63       285\n",
      "weighted avg       0.73      0.71      0.72       285\n",
      "\n",
      "[[144  20  22]\n",
      " [  2  16  10]\n",
      " [ 27   1  43]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.43717277486910994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.62      0.57       223\n",
      "         1.0       0.00      0.00      0.00        82\n",
      "         2.0       0.58      0.38      0.46        77\n",
      "\n",
      "    accuracy                           0.44       382\n",
      "   macro avg       0.37      0.33      0.34       382\n",
      "weighted avg       0.42      0.44      0.42       382\n",
      "\n",
      "[[138  65  20]\n",
      " [ 81   0   1]\n",
      " [ 45   3  29]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.7041095890410959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.92      0.81       242\n",
      "         1.0       0.44      0.20      0.28        55\n",
      "         2.0       0.75      0.35      0.48        68\n",
      "\n",
      "    accuracy                           0.70       365\n",
      "   macro avg       0.64      0.49      0.52       365\n",
      "weighted avg       0.68      0.70      0.67       365\n",
      "\n",
      "[[222  12   8]\n",
      " [ 44  11   0]\n",
      " [ 42   2  24]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5921052631578947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.77      0.73       193\n",
      "         1.0       0.08      0.10      0.09        40\n",
      "         2.0       0.69      0.38      0.49        71\n",
      "\n",
      "    accuracy                           0.59       304\n",
      "   macro avg       0.49      0.42      0.44       304\n",
      "weighted avg       0.61      0.59      0.59       304\n",
      "\n",
      "[[149  32  12]\n",
      " [ 36   4   0]\n",
      " [ 29  15  27]]\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.782608695652174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.82      0.86       191\n",
      "         1.0       0.33      0.04      0.07        25\n",
      "         2.0       0.62      0.92      0.74        83\n",
      "\n",
      "    accuracy                           0.78       299\n",
      "   macro avg       0.62      0.59      0.56       299\n",
      "weighted avg       0.78      0.78      0.76       299\n",
      "\n",
      "[[157   2  32]\n",
      " [ 10   1  14]\n",
      " [  7   0  76]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.712280701754386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.77      0.80       186\n",
      "         1.0       0.43      0.57      0.49        28\n",
      "         2.0       0.57      0.61      0.59        71\n",
      "\n",
      "    accuracy                           0.71       285\n",
      "   macro avg       0.61      0.65      0.63       285\n",
      "weighted avg       0.73      0.71      0.72       285\n",
      "\n",
      "[[144  20  22]\n",
      " [  2  16  10]\n",
      " [ 27   1  43]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.43717277486910994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.62      0.57       223\n",
      "         1.0       0.00      0.00      0.00        82\n",
      "         2.0       0.58      0.38      0.46        77\n",
      "\n",
      "    accuracy                           0.44       382\n",
      "   macro avg       0.37      0.33      0.34       382\n",
      "weighted avg       0.42      0.44      0.42       382\n",
      "\n",
      "[[138  65  20]\n",
      " [ 81   0   1]\n",
      " [ 45   3  29]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.7041095890410959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.92      0.81       242\n",
      "         1.0       0.44      0.20      0.28        55\n",
      "         2.0       0.75      0.35      0.48        68\n",
      "\n",
      "    accuracy                           0.70       365\n",
      "   macro avg       0.64      0.49      0.52       365\n",
      "weighted avg       0.68      0.70      0.67       365\n",
      "\n",
      "[[222  12   8]\n",
      " [ 44  11   0]\n",
      " [ 42   2  24]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5921052631578947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.77      0.73       193\n",
      "         1.0       0.08      0.10      0.09        40\n",
      "         2.0       0.69      0.38      0.49        71\n",
      "\n",
      "    accuracy                           0.59       304\n",
      "   macro avg       0.49      0.42      0.44       304\n",
      "weighted avg       0.61      0.59      0.59       304\n",
      "\n",
      "[[149  32  12]\n",
      " [ 36   4   0]\n",
      " [ 29  15  27]]\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.22911051212938005\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.91      0.33        77\n",
      "         1.0       0.40      0.07      0.12        56\n",
      "         2.0       0.58      0.05      0.09       238\n",
      "\n",
      "    accuracy                           0.23       371\n",
      "   macro avg       0.39      0.34      0.18       371\n",
      "weighted avg       0.47      0.23      0.14       371\n",
      "\n",
      "[[ 70   0   7]\n",
      " [ 51   4   1]\n",
      " [221   6  11]]\n",
      "User out:  2\n",
      "Shuffle data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.538860103626943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.71      0.69       283\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.08      0.06      0.07       103\n",
      "\n",
      "    accuracy                           0.54       386\n",
      "   macro avg       0.25      0.26      0.25       386\n",
      "weighted avg       0.52      0.54      0.53       386\n",
      "\n",
      "[[202   8  73]\n",
      " [  0   0   0]\n",
      " [ 97   0   6]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.4642857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.62      0.52       231\n",
      "         1.0       0.08      0.03      0.04        70\n",
      "         2.0       0.54      0.43      0.48       203\n",
      "\n",
      "    accuracy                           0.46       504\n",
      "   macro avg       0.36      0.36      0.35       504\n",
      "weighted avg       0.44      0.46      0.44       504\n",
      "\n",
      "[[144  19  68]\n",
      " [ 62   2   6]\n",
      " [112   3  88]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.3299798792756539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.47      0.44       271\n",
      "         1.0       0.40      0.20      0.27        50\n",
      "         2.0       0.16      0.15      0.15       176\n",
      "\n",
      "    accuracy                           0.33       497\n",
      "   macro avg       0.32      0.27      0.29       497\n",
      "weighted avg       0.32      0.33      0.32       497\n",
      "\n",
      "[[128  15 128]\n",
      " [ 31  10   9]\n",
      " [150   0  26]]\n",
      "User out:  5\n",
      "Shuffle data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4896755162241888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.82      0.64       181\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.41      0.11      0.17       158\n",
      "\n",
      "    accuracy                           0.49       339\n",
      "   macro avg       0.31      0.31      0.27       339\n",
      "weighted avg       0.47      0.49      0.42       339\n",
      "\n",
      "[[149   8  24]\n",
      " [  0   0   0]\n",
      " [135   6  17]]\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.22911051212938005\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.91      0.33        77\n",
      "         1.0       0.40      0.07      0.12        56\n",
      "         2.0       0.58      0.05      0.09       238\n",
      "\n",
      "    accuracy                           0.23       371\n",
      "   macro avg       0.39      0.34      0.18       371\n",
      "weighted avg       0.47      0.23      0.14       371\n",
      "\n",
      "[[ 70   0   7]\n",
      " [ 51   4   1]\n",
      " [221   6  11]]\n",
      "User out:  2\n",
      "Shuffle data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.538860103626943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.71      0.69       283\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.08      0.06      0.07       103\n",
      "\n",
      "    accuracy                           0.54       386\n",
      "   macro avg       0.25      0.26      0.25       386\n",
      "weighted avg       0.52      0.54      0.53       386\n",
      "\n",
      "[[202   8  73]\n",
      " [  0   0   0]\n",
      " [ 97   0   6]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.4642857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.62      0.52       231\n",
      "         1.0       0.08      0.03      0.04        70\n",
      "         2.0       0.54      0.43      0.48       203\n",
      "\n",
      "    accuracy                           0.46       504\n",
      "   macro avg       0.36      0.36      0.35       504\n",
      "weighted avg       0.44      0.46      0.44       504\n",
      "\n",
      "[[144  19  68]\n",
      " [ 62   2   6]\n",
      " [112   3  88]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.3299798792756539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.47      0.44       271\n",
      "         1.0       0.40      0.20      0.27        50\n",
      "         2.0       0.16      0.15      0.15       176\n",
      "\n",
      "    accuracy                           0.33       497\n",
      "   macro avg       0.32      0.27      0.29       497\n",
      "weighted avg       0.32      0.33      0.32       497\n",
      "\n",
      "[[128  15 128]\n",
      " [ 31  10   9]\n",
      " [150   0  26]]\n",
      "User out:  5\n",
      "Shuffle data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4896755162241888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.82      0.64       181\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.41      0.11      0.17       158\n",
      "\n",
      "    accuracy                           0.49       339\n",
      "   macro avg       0.31      0.31      0.27       339\n",
      "weighted avg       0.47      0.49      0.42       339\n",
      "\n",
      "[[149   8  24]\n",
      " [  0   0   0]\n",
      " [135   6  17]]\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.49053030303030304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.56      0.63       385\n",
      "         1.0       0.35      0.55      0.42        44\n",
      "         2.0       0.12      0.19      0.15        99\n",
      "\n",
      "    accuracy                           0.49       528\n",
      "   macro avg       0.39      0.43      0.40       528\n",
      "weighted avg       0.57      0.49      0.52       528\n",
      "\n",
      "[[216  39 130]\n",
      " [ 16  24   4]\n",
      " [ 74   6  19]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.43097643097643096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.63      0.60       180\n",
      "         1.0       0.06      0.08      0.07        37\n",
      "         2.0       0.24      0.15      0.18        80\n",
      "\n",
      "    accuracy                           0.43       297\n",
      "   macro avg       0.29      0.29      0.28       297\n",
      "weighted avg       0.42      0.43      0.42       297\n",
      "\n",
      "[[113  41  26]\n",
      " [ 21   3  13]\n",
      " [ 65   3  12]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.5336927223719676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.65      0.71       260\n",
      "         1.0       0.05      0.09      0.06        35\n",
      "         2.0       0.28      0.33      0.30        76\n",
      "\n",
      "    accuracy                           0.53       371\n",
      "   macro avg       0.36      0.36      0.36       371\n",
      "weighted avg       0.60      0.53      0.56       371\n",
      "\n",
      "[[170  36  54]\n",
      " [ 21   3  11]\n",
      " [ 31  20  25]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.7134502923976608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.87      0.83       218\n",
      "         1.0       0.41      0.54      0.47        37\n",
      "         2.0       0.66      0.40      0.50        87\n",
      "\n",
      "    accuracy                           0.71       342\n",
      "   macro avg       0.62      0.60      0.60       342\n",
      "weighted avg       0.71      0.71      0.70       342\n",
      "\n",
      "[[189  13  16]\n",
      " [ 15  20   2]\n",
      " [ 36  16  35]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.2876712328767123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.36      0.42       195\n",
      "         1.0       0.26      0.29      0.28        31\n",
      "         2.0       0.03      0.06      0.04        66\n",
      "\n",
      "    accuracy                           0.29       292\n",
      "   macro avg       0.27      0.24      0.25       292\n",
      "weighted avg       0.37      0.29      0.32       292\n",
      "\n",
      "[[ 71  19 105]\n",
      " [ 15   9   7]\n",
      " [ 56   6   4]]\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.49053030303030304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.56      0.63       385\n",
      "         1.0       0.35      0.55      0.42        44\n",
      "         2.0       0.12      0.19      0.15        99\n",
      "\n",
      "    accuracy                           0.49       528\n",
      "   macro avg       0.39      0.43      0.40       528\n",
      "weighted avg       0.57      0.49      0.52       528\n",
      "\n",
      "[[216  39 130]\n",
      " [ 16  24   4]\n",
      " [ 74   6  19]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.43097643097643096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.63      0.60       180\n",
      "         1.0       0.06      0.08      0.07        37\n",
      "         2.0       0.24      0.15      0.18        80\n",
      "\n",
      "    accuracy                           0.43       297\n",
      "   macro avg       0.29      0.29      0.28       297\n",
      "weighted avg       0.42      0.43      0.42       297\n",
      "\n",
      "[[113  41  26]\n",
      " [ 21   3  13]\n",
      " [ 65   3  12]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.5336927223719676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.65      0.71       260\n",
      "         1.0       0.05      0.09      0.06        35\n",
      "         2.0       0.28      0.33      0.30        76\n",
      "\n",
      "    accuracy                           0.53       371\n",
      "   macro avg       0.36      0.36      0.36       371\n",
      "weighted avg       0.60      0.53      0.56       371\n",
      "\n",
      "[[170  36  54]\n",
      " [ 21   3  11]\n",
      " [ 31  20  25]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.7134502923976608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.87      0.83       218\n",
      "         1.0       0.41      0.54      0.47        37\n",
      "         2.0       0.66      0.40      0.50        87\n",
      "\n",
      "    accuracy                           0.71       342\n",
      "   macro avg       0.62      0.60      0.60       342\n",
      "weighted avg       0.71      0.71      0.70       342\n",
      "\n",
      "[[189  13  16]\n",
      " [ 15  20   2]\n",
      " [ 36  16  35]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.2876712328767123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.36      0.42       195\n",
      "         1.0       0.26      0.29      0.28        31\n",
      "         2.0       0.03      0.06      0.04        66\n",
      "\n",
      "    accuracy                           0.29       292\n",
      "   macro avg       0.27      0.24      0.25       292\n",
      "weighted avg       0.37      0.29      0.32       292\n",
      "\n",
      "[[ 71  19 105]\n",
      " [ 15   9   7]\n",
      " [ 56   6   4]]\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.7284768211920529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81        96\n",
      "         1.0       0.00      0.00      0.00        13\n",
      "         2.0       0.63      0.81      0.71        42\n",
      "\n",
      "    accuracy                           0.73       151\n",
      "   macro avg       0.49      0.53      0.51       151\n",
      "weighted avg       0.70      0.73      0.71       151\n",
      "\n",
      "[[76  5 15]\n",
      " [ 8  0  5]\n",
      " [ 8  0 34]]\n",
      "User out:  2\n",
      "Shuffle data\n",
      "Accuracy: 0.6458333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.71      0.74        94\n",
      "         1.0       0.18      0.29      0.22        14\n",
      "         2.0       0.63      0.61      0.62        36\n",
      "\n",
      "    accuracy                           0.65       144\n",
      "   macro avg       0.53      0.54      0.53       144\n",
      "weighted avg       0.68      0.65      0.66       144\n",
      "\n",
      "[[67 16 11]\n",
      " [ 8  4  2]\n",
      " [12  2 22]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.46632124352331605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.67      0.61       113\n",
      "         1.0       0.00      0.00      0.00        41\n",
      "         2.0       0.42      0.36      0.39        39\n",
      "\n",
      "    accuracy                           0.47       193\n",
      "   macro avg       0.32      0.34      0.33       193\n",
      "weighted avg       0.41      0.47      0.43       193\n",
      "\n",
      "[[76 19 18]\n",
      " [40  0  1]\n",
      " [22  3 14]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.5513513513513514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.74      0.70       122\n",
      "         1.0       0.04      0.04      0.04        28\n",
      "         2.0       0.44      0.31      0.37        35\n",
      "\n",
      "    accuracy                           0.55       185\n",
      "   macro avg       0.38      0.36      0.37       185\n",
      "weighted avg       0.53      0.55      0.54       185\n",
      "\n",
      "[[90 18 14]\n",
      " [27  1  0]\n",
      " [18  6 11]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5555555555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.73      0.69        97\n",
      "         1.0       0.04      0.05      0.05        20\n",
      "         2.0       0.68      0.36      0.47        36\n",
      "\n",
      "    accuracy                           0.56       153\n",
      "   macro avg       0.46      0.38      0.40       153\n",
      "weighted avg       0.58      0.56      0.55       153\n",
      "\n",
      "[[71 20  6]\n",
      " [19  1  0]\n",
      " [20  3 13]]\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.7284768211920529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81        96\n",
      "         1.0       0.00      0.00      0.00        13\n",
      "         2.0       0.63      0.81      0.71        42\n",
      "\n",
      "    accuracy                           0.73       151\n",
      "   macro avg       0.49      0.53      0.51       151\n",
      "weighted avg       0.70      0.73      0.71       151\n",
      "\n",
      "[[76  5 15]\n",
      " [ 8  0  5]\n",
      " [ 8  0 34]]\n",
      "User out:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data\n",
      "Accuracy: 0.6458333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.71      0.74        94\n",
      "         1.0       0.18      0.29      0.22        14\n",
      "         2.0       0.63      0.61      0.62        36\n",
      "\n",
      "    accuracy                           0.65       144\n",
      "   macro avg       0.53      0.54      0.53       144\n",
      "weighted avg       0.68      0.65      0.66       144\n",
      "\n",
      "[[67 16 11]\n",
      " [ 8  4  2]\n",
      " [12  2 22]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.46632124352331605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.67      0.61       113\n",
      "         1.0       0.00      0.00      0.00        41\n",
      "         2.0       0.42      0.36      0.39        39\n",
      "\n",
      "    accuracy                           0.47       193\n",
      "   macro avg       0.32      0.34      0.33       193\n",
      "weighted avg       0.41      0.47      0.43       193\n",
      "\n",
      "[[76 19 18]\n",
      " [40  0  1]\n",
      " [22  3 14]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.5513513513513514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.74      0.70       122\n",
      "         1.0       0.04      0.04      0.04        28\n",
      "         2.0       0.44      0.31      0.37        35\n",
      "\n",
      "    accuracy                           0.55       185\n",
      "   macro avg       0.38      0.36      0.37       185\n",
      "weighted avg       0.53      0.55      0.54       185\n",
      "\n",
      "[[90 18 14]\n",
      " [27  1  0]\n",
      " [18  6 11]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.5555555555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.73      0.69        97\n",
      "         1.0       0.04      0.05      0.05        20\n",
      "         2.0       0.68      0.36      0.47        36\n",
      "\n",
      "    accuracy                           0.56       153\n",
      "   macro avg       0.46      0.38      0.40       153\n",
      "weighted avg       0.58      0.56      0.55       153\n",
      "\n",
      "[[71 20  6]\n",
      " [19  1  0]\n",
      " [20  3 13]]\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.23655913978494625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.85      0.33        39\n",
      "         1.0       0.00      0.00      0.00        28\n",
      "         2.0       0.58      0.09      0.16       119\n",
      "\n",
      "    accuracy                           0.24       186\n",
      "   macro avg       0.26      0.31      0.16       186\n",
      "weighted avg       0.41      0.24      0.17       186\n",
      "\n",
      "[[ 33   0   6]\n",
      " [ 26   0   2]\n",
      " [103   5  11]]\n",
      "User out:  2\n",
      "Shuffle data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5412371134020618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.73      0.70       142\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.06      0.04      0.05        52\n",
      "\n",
      "    accuracy                           0.54       194\n",
      "   macro avg       0.25      0.25      0.25       194\n",
      "weighted avg       0.51      0.54      0.53       194\n",
      "\n",
      "[[103   7  32]\n",
      " [  0   0   0]\n",
      " [ 49   1   2]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.42292490118577075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.63      0.51       116\n",
      "         1.0       0.27      0.11      0.16        35\n",
      "         2.0       0.45      0.29      0.36       102\n",
      "\n",
      "    accuracy                           0.42       253\n",
      "   macro avg       0.38      0.35      0.34       253\n",
      "weighted avg       0.41      0.42      0.40       253\n",
      "\n",
      "[[73 11 32]\n",
      " [27  4  4]\n",
      " [72  0 30]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.3815261044176707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.46      0.45       136\n",
      "         1.0       0.38      0.12      0.18        25\n",
      "         2.0       0.29      0.33      0.31        88\n",
      "\n",
      "    accuracy                           0.38       249\n",
      "   macro avg       0.37      0.30      0.32       249\n",
      "weighted avg       0.38      0.38      0.38       249\n",
      "\n",
      "[[63  4 69]\n",
      " [21  3  1]\n",
      " [58  1 29]]\n",
      "User out:  5\n",
      "Shuffle data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4678362573099415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.79      0.62        91\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.33      0.10      0.15        80\n",
      "\n",
      "    accuracy                           0.47       171\n",
      "   macro avg       0.28      0.30      0.26       171\n",
      "weighted avg       0.43      0.47      0.40       171\n",
      "\n",
      "[[72  3 16]\n",
      " [ 0  0  0]\n",
      " [70  2  8]]\n",
      "User out:  1\n",
      "Shuffle data\n",
      "Accuracy: 0.23655913978494625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.85      0.33        39\n",
      "         1.0       0.00      0.00      0.00        28\n",
      "         2.0       0.58      0.09      0.16       119\n",
      "\n",
      "    accuracy                           0.24       186\n",
      "   macro avg       0.26      0.31      0.16       186\n",
      "weighted avg       0.41      0.24      0.17       186\n",
      "\n",
      "[[ 33   0   6]\n",
      " [ 26   0   2]\n",
      " [103   5  11]]\n",
      "User out:  2\n",
      "Shuffle data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5412371134020618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.73      0.70       142\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.06      0.04      0.05        52\n",
      "\n",
      "    accuracy                           0.54       194\n",
      "   macro avg       0.25      0.25      0.25       194\n",
      "weighted avg       0.51      0.54      0.53       194\n",
      "\n",
      "[[103   7  32]\n",
      " [  0   0   0]\n",
      " [ 49   1   2]]\n",
      "User out:  3\n",
      "Shuffle data\n",
      "Accuracy: 0.42292490118577075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.63      0.51       116\n",
      "         1.0       0.27      0.11      0.16        35\n",
      "         2.0       0.45      0.29      0.36       102\n",
      "\n",
      "    accuracy                           0.42       253\n",
      "   macro avg       0.38      0.35      0.34       253\n",
      "weighted avg       0.41      0.42      0.40       253\n",
      "\n",
      "[[73 11 32]\n",
      " [27  4  4]\n",
      " [72  0 30]]\n",
      "User out:  4\n",
      "Shuffle data\n",
      "Accuracy: 0.3815261044176707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.46      0.45       136\n",
      "         1.0       0.38      0.12      0.18        25\n",
      "         2.0       0.29      0.33      0.31        88\n",
      "\n",
      "    accuracy                           0.38       249\n",
      "   macro avg       0.37      0.30      0.32       249\n",
      "weighted avg       0.38      0.38      0.38       249\n",
      "\n",
      "[[63  4 69]\n",
      " [21  3  1]\n",
      " [58  1 29]]\n",
      "User out:  5\n",
      "Shuffle data\n",
      "Accuracy: 0.4678362573099415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.79      0.62        91\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.33      0.10      0.15        80\n",
      "\n",
      "    accuracy                           0.47       171\n",
      "   macro avg       0.28      0.30      0.26       171\n",
      "weighted avg       0.43      0.47      0.40       171\n",
      "\n",
      "[[72  3 16]\n",
      " [ 0  0  0]\n",
      " [70  2  8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr 17 17:00:02 2019\n",
    "\n",
    "@author: xngu0004\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "list_skill = [\"Suture\", \"Knot\", \"Needle\"]\n",
    "list_win = [90, 120, 150, 180]\n",
    "list_over = [30, 60]\n",
    "list_netw = [1]\n",
    "\n",
    "# users in Group\n",
    "novice = ['B', 'G', 'H', 'I']\n",
    "intermediate = ['C', 'F']\n",
    "expert = ['D', 'E']\n",
    "\n",
    "f_s = 30 # Sampling rate\n",
    "denominator = 10\n",
    "\n",
    "# Function to calculate FFT coefficiences\n",
    "def get_dct_values(y_values, T, N, f_s):\n",
    "    f_values = np.linspace(0.0, 1.0/(T), N)\n",
    "    dct_values_ = dct(y_values,3)\n",
    "    dct_values = 1.0/N * np.abs(dct_values_[0:N//2])\n",
    "    return f_values, dct_values\n",
    "\n",
    "# Revised from http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n",
    "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising', kpsh=False, valley=False, ax=None):\n",
    "\n",
    "    x = np.atleast_1d(x).astype('float64')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    # handle NaN's\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    # handle NaN's\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size-1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                    & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    return ind\n",
    "\n",
    "# Extract no_peaks\n",
    "def get_first_n_peaks(x,y,no_peaks=10):\n",
    "    x_, y_ = list(x), list(y)\n",
    "    if len(x_) >= no_peaks:\n",
    "        return x_[:no_peaks], y_[:no_peaks]\n",
    "    else:\n",
    "        missing_no_peaks = no_peaks-len(x_)\n",
    "        return x_ + [0]*missing_no_peaks, y_ + [0]*missing_no_peaks\n",
    "    \n",
    "def get_features(x_values, y_values, mph):\n",
    "    indices_peaks = detect_peaks(y_values, mph=mph)\n",
    "    peaks_x, peaks_y = get_first_n_peaks(x_values[indices_peaks], y_values[indices_peaks])\n",
    "    return peaks_x + peaks_y\n",
    "\n",
    "# Extract features\n",
    "def extract_features_labels(dataset, labels, T, N, f_s, denominator):\n",
    "    percentile = 5\n",
    "    list_of_features = []\n",
    "    list_of_labels = []\n",
    "    for signal_no in range(0, len(dataset)):\n",
    "        features = []\n",
    "        list_of_labels.append(labels[signal_no])\n",
    "        for signal_comp in range(0,dataset.shape[2]):\n",
    "            signal = dataset[signal_no, :, signal_comp]\n",
    "            \n",
    "            signal_min = np.nanpercentile(signal, percentile)\n",
    "            signal_max = np.nanpercentile(signal, 100-percentile)\n",
    "            ##ijk = (100 - 2*percentile)/10\n",
    "            mph = signal_min + (signal_max - signal_min)/denominator\n",
    "            \n",
    "            #features += get_features(*get_psd_values(signal, T, N, f_s), mph)\n",
    "            #features += get_features(*get_fft_values(signal, T, N, f_s), mph)\n",
    "            features += get_features(*get_dct_values(signal, T, N, f_s), mph)\n",
    "            #features += get_features(*get_autocorr_values(signal, T, N, f_s), mph)\n",
    "        list_of_features.append(features)\n",
    "    return np.array(list_of_features), np.array(list_of_labels)\n",
    "\n",
    "for win in list_win:\n",
    "    for over in list_over:\n",
    "        f_s = over #(it is set to 30 by default, but I noticed that maybe it needs to switch to 60 when over is 60)\n",
    "        for skill in list_skill:\n",
    "            \" Load the data\"\n",
    "            print('loading the dataset')\n",
    "            dataset = sio.loadmat(input_root + skill+'_dataX_basic_'+str(win)+'_'+str(over)+'.mat')\n",
    "            features = dataset['dataX']\n",
    "            sizeD_ts = (((features[0,0:1])[0])[0,:])[0].shape[0]\n",
    "            sizeD_va = (((features[0,0:1])[0])[0,:])[0].shape[1]\n",
    "            N = sizeD_ts # Number of samples\n",
    "            t_n = N/f_s # Duration of window\n",
    "            T = 1 / f_s # Period\n",
    "            print('loaded the dataset')\n",
    "            for netw in list_netw:\n",
    "                for tim in range(3,5):\n",
    "                    full_output_path = output_root + 'DCT/' + str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+'_'+str(netw)+'_report_DCT_fea.txt'\n",
    "                    if generateOnlyMissingResultFiles and path.exists(full_output_path):\n",
    "                        print(full_output_path + ' already exists')\n",
    "                        continue\n",
    "                    acc_1 = []\n",
    "                    accu = 0\n",
    "                    acc_1_fea = []\n",
    "                    accu_fea = 0                    \n",
    "                    ###################################\n",
    "                    #Pre-process data\n",
    "                    for user in range(1,6):\n",
    "                        \" Initialize data for concatenation \"\n",
    "                        X_test1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "                        y_test1 = np.ones(1)\n",
    "                        X_train1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "                        y_train1 = np.ones(1)\n",
    "                        print(\"User out: \", user)\n",
    "                        # Extract data for training and testing\n",
    "                        for j in groupRange:\n",
    "                            for i in range(0, (((features[0,j:j+1])[0])[0,:]).shape[0]):\n",
    "                                k = (((features[0,j:j+1])[0])[2,:])[i]\n",
    "                                if (k == user): # take out the ith trial of each user for testing\n",
    "                                    temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                                    temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                                    X_test1 = np.concatenate((X_test1, temp_x1), axis=0) \n",
    "                                    user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                                    if(user_temp in novice):\n",
    "                                        y_test1 = np.concatenate((y_test1,[0]))\n",
    "                                    elif(user_temp in intermediate):\n",
    "                                        y_test1 = np.concatenate((y_test1,[1]))\n",
    "                                    elif(user_temp in expert):\n",
    "                                        y_test1 = np.concatenate((y_test1,[2]))\n",
    "                                else:\n",
    "                                    temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                                    temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                                    X_train1 = np.concatenate((X_train1, temp_x1), axis=0) \n",
    "                                    user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                                    if(user_temp in novice):\n",
    "                                        y_train1 = np.concatenate((y_train1,[0]))\n",
    "                                    elif(user_temp in intermediate):\n",
    "                                        y_train1 = np.concatenate((y_train1,[1]))\n",
    "                                    elif(user_temp in expert):\n",
    "                                        y_train1 = np.concatenate((y_train1,[2]))\n",
    "        \n",
    "                        \" Delete the first column \"\n",
    "                        X_test1 = np.delete(X_test1,np.s_[0:1], axis=0)\n",
    "                        y_test1 = np.delete(y_test1,np.s_[0:1])\n",
    "                        X_train1 = np.delete(X_train1,np.s_[0:1], axis=0)\n",
    "                        y_train1= np.delete(y_train1,np.s_[0:1])\n",
    "                        \n",
    "                        print(\"Shuffle data\")\n",
    "                        \" Shuffle the training and testing data \"\n",
    "                        Xx_test, yy_test = shuffle(X_test1, y_test1, random_state = random.randint(10,50))\n",
    "                        Xx_train, yy_train = shuffle(X_train1, y_train1, random_state = random.randint(10,50))\n",
    "            \n",
    "                        ##############################################\n",
    "                        X_train, Y_train = extract_features_labels(Xx_train, yy_train, T, N, f_s, denominator)\n",
    "                        X_test, Y_test = extract_features_labels(Xx_test, yy_test, T, N, f_s, denominator)\n",
    "                        ##############################################\n",
    "                        \n",
    "                        svm1 = svm.SVC(kernel = 'linear')\n",
    "                        svm1.fit(X_train, Y_train)\n",
    "                        prediction_fea = svm1.predict(X_test)\n",
    "                        y_pred_fea = prediction_fea\n",
    "                        cr_fea = classification_report(Y_test, y_pred_fea)\n",
    "                        cm_fea = confusion_matrix(Y_test, y_pred_fea)\n",
    "    \n",
    "                        acc_fea = np.sum(y_pred_fea == Y_test)/y_pred_fea.shape[0]\n",
    "                        acc_1_fea.append(acc_fea)\n",
    "                        print('Accuracy: ' + str(acc_fea))\n",
    "                        print(cr_fea)\n",
    "                        print(cm_fea)\n",
    "                        f = open(full_output_path, 'a+')\n",
    "                        f.write(\"-------------------- DCT Features -----------------------\")\n",
    "                        f.write('---------' + skill + \" Win: \" + str(win) + \" Over: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "                        f.write('Accuracy: ' + str(acc_fea))\n",
    "                        f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr_fea, cm_fea))\n",
    "                    for i in range(0,5):\n",
    "                        accu_fea += acc_1_fea[i]\n",
    "                    accu_fea = accu_fea/5\n",
    "                    f.write(\"\\n-------------------------------------------\\n\")\n",
    "                    f.write(\"\\nAcc_features = \" + str(accu_fea))\n",
    "                    f.write(\"\\n-------------------------------------------\\n\")\n",
    "                    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gabor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gabor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gabor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gabor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gabor/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gabor/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import visualkeras\n",
    "import scipy.io as sio\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, BatchNormalization, Activation\n",
    "from keras.layers import LSTM, add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import svm\n",
    "\n",
    "input_root = '../data/augment_data_from_csvs/no_interm/'\n",
    "win = 180\n",
    "over = 30\n",
    "skill = 'Knot'\n",
    "\n",
    "dataset = sio.loadmat(input_root+skill+'_dataX_basic_'+str(win)+'_'+str(over)+'.mat')\n",
    "\n",
    "features = dataset['dataX']\n",
    "#print(features[0][1].shape, features[1][1].shape, features[2][1].shape)\n",
    "#sizeD_ts = features[0][1][0][1][0][0][0][0][0].shape[0]\n",
    "#sizeD_va = features[0][1][0][1][0][0][0][0][0].shape[1]\n",
    "sizeD_ts = (((features[0,0:1])[0])[0,:])[0].shape[0]\n",
    "sizeD_va = (((features[0,0:1])[0])[0,:])[0].shape[1]\n",
    "\n",
    "\n",
    "#######---------Resnet------------\n",
    "ip = Input(shape=(sizeD_ts, sizeD_va), name='main_input')\n",
    "\n",
    "##### Block 1\n",
    "x1 = Conv1D(64, 8, padding='same', kernel_initializer='he_uniform')(ip)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Activation('relu')(x1)\n",
    "\n",
    "x2 = Conv1D(64, 5, padding='same', kernel_initializer='he_uniform')(x1)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('relu')(x2)\n",
    "\n",
    "x3 = Conv1D(64, 3, padding='same', kernel_initializer='he_uniform')(x2)\n",
    "x3 = BatchNormalization()(x3)\n",
    "\n",
    "# expand channels for the sum\n",
    "sx2 = Conv1D(64, 1, padding='same', kernel_initializer='he_uniform')(ip)\n",
    "sx2 = BatchNormalization()(sx2)\n",
    "\n",
    "outputB1 = add([sx2, x3])\n",
    "outputB1 = Activation('relu')(outputB1)\n",
    "\n",
    "##### Block 2\n",
    "x1 = Conv1D(64, 8, padding='same', kernel_initializer='he_uniform')(outputB1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Activation('relu')(x1)\n",
    "\n",
    "x2 = Conv1D(64, 5, padding='same', kernel_initializer='he_uniform')(x1)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('relu')(x2)\n",
    "\n",
    "x3 = Conv1D(64, 3, padding='same', kernel_initializer='he_uniform')(x2)\n",
    "x3 = BatchNormalization()(x3)\n",
    "\n",
    "# expand channels for the sum\n",
    "sx2 = Conv1D(64, 1, padding='same', kernel_initializer='he_uniform')(outputB1)\n",
    "sx2 = BatchNormalization()(sx2)\n",
    "\n",
    "outputB2 = add([sx2, x3])\n",
    "outputB2 = Activation('relu')(outputB2)\n",
    "\n",
    "##### Block 3\n",
    "x1 = Conv1D(64, 8, padding='same', kernel_initializer='he_uniform')(outputB2)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Activation('relu')(x1)\n",
    "\n",
    "x2 = Conv1D(64, 5, padding='same', kernel_initializer='he_uniform')(x1)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('relu')(x2)\n",
    "\n",
    "x3 = Conv1D(64, 3, padding='same', kernel_initializer='he_uniform')(x2)\n",
    "x3 = BatchNormalization()(x3)\n",
    "\n",
    "#do not need to expand channels for the sum\n",
    "sx2 = BatchNormalization()(outputB2)\n",
    "\n",
    "outputB3 = add([sx2, x3])\n",
    "outputB3 = Activation('relu')(outputB2)\n",
    "\n",
    "#### End\n",
    "fea = GlobalAveragePooling1D()(outputB3)\n",
    "out = Dense(3, kernel_regularizer=regularizers.l2(0.001), activation='softmax')(fea)\n",
    "\n",
    "model = Model(ip, out)\n",
    "\n",
    "visualkeras.layered_view(model, to_file='resnet_model.png').show() # write and show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three user groups (Beginner, Intermediate, Expert) are processed\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['../data/augment_data_from_csvs/',\n",
    "          '../data/augment_data_from_csvs/no_interm/',\n",
    "]\n",
    "\n",
    "outputs = ['../results/csv_based/',\n",
    "           '../results/csv_without_interms/',\n",
    "]\n",
    "\n",
    "ind = 1\n",
    "\n",
    "withoutInterms = (ind == 1)\n",
    "\n",
    "output_root = outputs[ind]\n",
    "input_root = inputs[ind]\n",
    "\n",
    "if withoutInterms:\n",
    "    print('Intermediate user data will be skipped')\n",
    "else:\n",
    "    print('All three user groups (Beginner, Intermediate, Expert) are processed')\n",
    "    \n",
    "groupRange = [0,2]\n",
    "if not withoutInterms:\n",
    "    groupRange = [0,1,2]\n",
    "print(groupRange)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "generateOnlyMissingResultFiles = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/csv_based/ResNet/1_Suture_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/2_Suture_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/3_Suture_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/4_Suture_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/5_Suture_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/1_Knot_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/2_Knot_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/3_Knot_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/4_Knot_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/5_Knot_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/1_Needle_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/2_Needle_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/3_Needle_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/4_Needle_150_60_report_Resnet_fea.txt already exists\n",
      "../results/csv_based/ResNet/5_Needle_150_60_report_Resnet_fea.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Mar 17 18:55:21 2019\n",
    "\n",
    "@author: xngu0004\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, BatchNormalization, Activation\n",
    "from keras.layers import LSTM, add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import svm\n",
    "\n",
    "#####################################################\n",
    "\"Select a configuration\"\n",
    "\n",
    "list_skill = [\"Suture\",\"Knot\",\"Needle\"]\n",
    "win = 150#[90,120,150,180]\n",
    "over = 60#[30,60]\n",
    "\n",
    "####################################################\n",
    "for skill in list_skill:\n",
    "    for tim in range(1,6):        \n",
    "        full_output_path = output_root + 'ResNet/' + str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+'_report_Resnet_fea.txt'\n",
    "        if generateOnlyMissingResultFiles and path.exists(full_output_path):\n",
    "            print(full_output_path + ' already exists')\n",
    "            continue\n",
    "        \n",
    "        acc_1 = []\n",
    "        accu = 0\n",
    "        acc_1_fea = []\n",
    "        accu_fea = 0\n",
    "        # users in Group\n",
    "        novice = ['B', 'G', 'H', 'I']\n",
    "        intermediate = ['C', 'F']\n",
    "        expert = ['D', 'E']\n",
    "\n",
    "        \" Load the data\"\n",
    "        \" Reading the data \"\n",
    "        print('loading the dataset')\n",
    "        dataset = sio.loadmat(input_root+skill+'_dataX_basic_'+str(win)+'_'+str(over)+'.mat')\n",
    "        \n",
    "        features = dataset['dataX']\n",
    "        sizeD_ts = (((features[0,0:1])[0])[0,:])[0].shape[0]\n",
    "        sizeD_va = (((features[0,0:1])[0])[0,:])[0].shape[1]\n",
    "        print('loaded the dataset')\n",
    "    \n",
    "        ###############################################################################\n",
    "\n",
    "        print('Pre-process data')\n",
    "        for user in range(1,6):\n",
    "\n",
    "            \" Initialize data for concatenation \"\n",
    "            X_test1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_test1 = np.ones(1)\n",
    "            X_train1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_train1 = np.ones(1)\n",
    "\n",
    "            # Extract data for training and testing\n",
    "            for j in groupRange:\n",
    "                upper_range = (((features[0,j:j+1])[0])[0,:]).shape[0]\n",
    "                for i in range(0, upper_range):\n",
    "                    k = (((features[0,j:j+1])[0])[2,:])[i]\n",
    "                    if (k == user): # take out the ith trial of each user for testing\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_test1 = np.concatenate((X_test1, temp_x1), axis=0) \n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        if(user_temp in novice):\n",
    "                            y_test1 = np.concatenate((y_test1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_test1 = np.concatenate((y_test1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_test1 = np.concatenate((y_test1,[2]))\n",
    "                    else:\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_train1 = np.concatenate((X_train1, temp_x1), axis=0)\n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        if(user_temp in novice):\n",
    "                            y_train1 = np.concatenate((y_train1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_train1 = np.concatenate((y_train1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_train1 = np.concatenate((y_train1,[2]))\n",
    "        \n",
    "            \" Delete the first column \"\n",
    "            X_test1 = np.delete(X_test1,np.s_[0:1], axis=0)\n",
    "            y_test1 = np.delete(y_test1,np.s_[0:1])\n",
    "            X_train1 = np.delete(X_train1,np.s_[0:1], axis=0)\n",
    "            y_train1 = np.delete(y_train1,np.s_[0:1])\n",
    "\n",
    "            \" Shuffle the training and testing data \"\n",
    "            X_test, y_test = shuffle(X_test1, y_test1, random_state = random.randint(10,50))\n",
    "            X_train, y_train = shuffle(X_train1, y_train1, random_state = random.randint(10,50))\n",
    "    \n",
    "            classes = np.unique(y_train)\n",
    "            le = LabelEncoder()\n",
    "            y_ind = le.fit_transform(y_train.ravel())\n",
    "            recip_freq = len(y_train) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n",
    "            class_weight = recip_freq[le.transform(classes)]\n",
    "    \n",
    "            \" One-hot coding\"\n",
    "            labels_train = to_categorical(y_train)\n",
    "            labels_test = to_categorical(y_test)\n",
    "\n",
    "            ###########################################################################\n",
    "            print(\"Ready to train\")\n",
    "            \n",
    "            #######---------Resnet------------\n",
    "            ip = Input(shape=(sizeD_ts, sizeD_va), name='main_input')\n",
    "    \n",
    "            ##### Block 1\n",
    "            x1 = Conv1D(64, 8, padding='same', kernel_initializer='he_uniform')(ip)\n",
    "            x1 = BatchNormalization()(x1)\n",
    "            x1 = Activation('relu')(x1)\n",
    "            \n",
    "            x2 = Conv1D(64, 5, padding='same', kernel_initializer='he_uniform')(x1)\n",
    "            x2 = BatchNormalization()(x2)\n",
    "            x2 = Activation('relu')(x2)\n",
    "            \n",
    "            x3 = Conv1D(64, 3, padding='same', kernel_initializer='he_uniform')(x2)\n",
    "            x3 = BatchNormalization()(x3)\n",
    "\n",
    "            # expand channels for the sum\n",
    "            sx2 = Conv1D(64, 1, padding='same', kernel_initializer='he_uniform')(ip)\n",
    "            sx2 = BatchNormalization()(sx2)\n",
    "            \n",
    "            outputB1 = add([sx2, x3])\n",
    "            outputB1 = Activation('relu')(outputB1)\n",
    "            \n",
    "            ##### Block 2\n",
    "            x1 = Conv1D(64, 8, padding='same', kernel_initializer='he_uniform')(outputB1)\n",
    "            x1 = BatchNormalization()(x1)\n",
    "            x1 = Activation('relu')(x1)\n",
    "            \n",
    "            x2 = Conv1D(64, 5, padding='same', kernel_initializer='he_uniform')(x1)\n",
    "            x2 = BatchNormalization()(x2)\n",
    "            x2 = Activation('relu')(x2)\n",
    "            \n",
    "            x3 = Conv1D(64, 3, padding='same', kernel_initializer='he_uniform')(x2)\n",
    "            x3 = BatchNormalization()(x3)\n",
    "\n",
    "            # expand channels for the sum\n",
    "            sx2 = Conv1D(64, 1, padding='same', kernel_initializer='he_uniform')(outputB1)\n",
    "            sx2 = BatchNormalization()(sx2)\n",
    "            \n",
    "            outputB2 = add([sx2, x3])\n",
    "            outputB2 = Activation('relu')(outputB2)\n",
    "            \n",
    "            ##### Block 3\n",
    "            x1 = Conv1D(64, 8, padding='same', kernel_initializer='he_uniform')(outputB2)\n",
    "            x1 = BatchNormalization()(x1)\n",
    "            x1 = Activation('relu')(x1)\n",
    "            \n",
    "            x2 = Conv1D(64, 5, padding='same', kernel_initializer='he_uniform')(x1)\n",
    "            x2 = BatchNormalization()(x2)\n",
    "            x2 = Activation('relu')(x2)\n",
    "            \n",
    "            x3 = Conv1D(64, 3, padding='same', kernel_initializer='he_uniform')(x2)\n",
    "            x3 = BatchNormalization()(x3)\n",
    "\n",
    "            #do not need to expand channels for the sum\n",
    "            sx2 = BatchNormalization()(outputB2)\n",
    "            \n",
    "            outputB3 = add([sx2, x3])\n",
    "            outputB3 = Activation('relu')(outputB2)\n",
    "            \n",
    "            #### End\n",
    "            fea = GlobalAveragePooling1D()(outputB3)\n",
    "            out = Dense(3, kernel_regularizer=regularizers.l2(0.001), activation='softmax')(fea)\n",
    "            \n",
    "            model = Model(ip, out)\n",
    "######---------------------\n",
    "            \n",
    "            epochs_s = 100\n",
    "            batch_s = 16\n",
    "            learning_rate = 1e-3   \n",
    "            weight_fn = \"./weights/weights_fea_Resnet_t\"+str(user)+\".h5\"\n",
    "            model_checkpoint = ModelCheckpoint(weight_fn, verbose=1, mode='max', monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "            stop = EarlyStopping(monitor='val_loss', patience=15)\n",
    "            callback_list = [model_checkpoint, stop]\n",
    "            optm = Adam(lr=learning_rate)\n",
    "\n",
    "            model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.fit(X_train, labels_train, batch_size=batch_s, epochs=epochs_s, callbacks = callback_list, class_weight=class_weight, verbose=2, validation_data=(X_test, labels_test))\n",
    "\n",
    "            model.load_weights(weight_fn)\n",
    "            prediction = model.predict(X_test);\n",
    "            y_pred = np.argmax(prediction, axis=1)\n",
    "    \n",
    "            cr = classification_report(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "            print(\"-------------------- Full NN -----------------------\")\n",
    "            print(\"Skill: \", skill)\n",
    "            print(\"Trial out:\", user)\n",
    "            acc = np.sum(y_pred == y_test)/y_pred.shape[0]\n",
    "            acc_1.append(acc)\n",
    "            print('Accuracy: ' + str(acc))\n",
    "            print(cr)\n",
    "            print(cm)\n",
    "            f = open(full_output_path, 'a+')\n",
    "            f.write(\"-------------------- Full NN -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr, cm))\n",
    "            \n",
    "            ##############################################\n",
    "            moFea = Model(ip, fea)\n",
    "            X_train_fea = moFea.predict(X_train)\n",
    "            X_test_fea = moFea.predict(X_test)\n",
    "            svm1 = svm.SVC(kernel = 'linear')\n",
    "            svm1.fit(X_train_fea, y_train)\n",
    "            prediction_fea = svm1.predict(X_test_fea)\n",
    "            y_pred_fea = prediction_fea\n",
    "            cr_fea = classification_report(y_test, y_pred_fea)\n",
    "            cm_fea = confusion_matrix(y_test, y_pred_fea)\n",
    "    \n",
    "            acc_fea = np.sum(y_pred_fea == y_test)/y_pred_fea.shape[0]\n",
    "            acc_1_fea.append(acc_fea)\n",
    "            print('Accuracy: ' + str(acc_fea))\n",
    "            print(cr_fea)\n",
    "            print(cm_fea)\n",
    "            f.write(\"-------------------- NN Fea -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc_fea))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr_fea, cm_fea))\n",
    "        for i in range(0,5):\n",
    "            accu += acc_1[i]\n",
    "            accu_fea += acc_1_fea[i]\n",
    "        accu = accu/5\n",
    "        accu_fea = accu_fea/5\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.write(\"Acc_full = \" + str(accu))\n",
    "        f.write(\"\\nAcc_fea = \" + str(accu_fea))\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three user groups (Beginner, Intermediate, Expert) are processed\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['../data/augment_data_from_csvs/',\n",
    "          '../data/augment_data_from_csvs/no_interm/',\n",
    "]\n",
    "\n",
    "outputs = ['../results/csv_based/',\n",
    "           '../results/csv_without_interms/',\n",
    "]\n",
    "\n",
    "ind = 1\n",
    "\n",
    "withoutInterms = (ind == 1)\n",
    "\n",
    "output_root = outputs[ind]\n",
    "input_root = inputs[ind]\n",
    "\n",
    "if withoutInterms:\n",
    "    print('Intermediate user data will be skipped')\n",
    "else:\n",
    "    print('All three user groups (Beginner, Intermediate, Expert) are processed')\n",
    "    \n",
    "groupRange = [0,2]\n",
    "if not withoutInterms:\n",
    "    groupRange = [0,1,2]\n",
    "print(groupRange)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "generateOnlyMissingResultFiles = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/csv_based/LSTM/1_Suture_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/2_Suture_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/3_Suture_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/4_Suture_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/5_Suture_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/1_Knot_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/2_Knot_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/3_Knot_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/4_Knot_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/5_Knot_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/1_Needle_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/2_Needle_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/3_Needle_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/4_Needle_180_30_report_LSTM_fea.txt already exists\n",
      "../results/csv_based/LSTM/5_Needle_180_30_report_LSTM_fea.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Apr 22 22:04:11 2019\n",
    "\n",
    "@author: xngu0004\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, BatchNormalization, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import svm\n",
    "\n",
    "#####################################################\n",
    "\"Select a configuration\"\n",
    "\n",
    "list_skill = [\"Suture\",\"Knot\",\"Needle\"]\n",
    "win = 180#[90,120,150,180]\n",
    "over = 30#[30,60]\n",
    "\n",
    "####################################################\n",
    "for skill in list_skill:\n",
    "    for tim in range(1,6):\n",
    "        full_output_path = output_root + 'LSTM/' + str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+'_report_LSTM_fea.txt'\n",
    "        if generateOnlyMissingResultFiles and path.exists(full_output_path):\n",
    "            print(full_output_path + ' already exists')\n",
    "            continue\n",
    "        acc_1 = []\n",
    "        accu = 0\n",
    "        acc_1_fea = []\n",
    "        accu_fea = 0\n",
    "        # users in Group\n",
    "        novice = ['B', 'G', 'H', 'I']\n",
    "        intermediate = ['C', 'F']\n",
    "        expert = ['D', 'E']\n",
    "\n",
    "        \" Load the data\"\n",
    "        \" Reading the data \"\n",
    "        print('loading the dataset')\n",
    "        dataset = sio.loadmat(input_root + skill+'_dataX_basic_'+str(win)+'_'+str(over)+'.mat')\n",
    "\n",
    "        features = dataset['dataX']\n",
    "        sizeD_ts = (((features[0,0:1])[0])[0,:])[0].shape[0]\n",
    "        sizeD_va = (((features[0,0:1])[0])[0,:])[0].shape[1]\n",
    "        print('loaded the dataset')\n",
    "    \n",
    "        ###############################################################################\n",
    "\n",
    "        print('Pre-process data')\n",
    "        for user in range(1,6):\n",
    "\n",
    "            \" Initialize data for concatenation \"\n",
    "            X_test1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_test1 = np.ones(1)\n",
    "            X_train1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_train1 = np.ones(1)\n",
    "\n",
    "            # Extract data for training and testing\n",
    "            for j in groupRange:\n",
    "                for i in range(0, (((features[0,j:j+1])[0])[0,:]).shape[0]):\n",
    "                    k = (((features[0,j:j+1])[0])[2,:])[i]\n",
    "                    if (k == user): # take out the ith trial of each user for testing\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_test1 = np.concatenate((X_test1, temp_x1), axis=0) \n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        if(user_temp in novice):\n",
    "                            y_test1 = np.concatenate((y_test1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_test1 = np.concatenate((y_test1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_test1 = np.concatenate((y_test1,[2]))\n",
    "                    else:\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_train1 = np.concatenate((X_train1, temp_x1), axis=0)\n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "        \n",
    "                        if(user_temp in novice):\n",
    "                            y_train1 = np.concatenate((y_train1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_train1 = np.concatenate((y_train1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_train1 = np.concatenate((y_train1,[2]))\n",
    "        \n",
    "            \" Delete the first column \"\n",
    "            X_test1 = np.delete(X_test1,np.s_[0:1], axis=0)\n",
    "            y_test1 = np.delete(y_test1,np.s_[0:1])\n",
    "            X_train1 = np.delete(X_train1,np.s_[0:1], axis=0)\n",
    "            y_train1 = np.delete(y_train1,np.s_[0:1])\n",
    "\n",
    "            \" Shuffle the training and testing data \"\n",
    "            X_test, y_test = shuffle(X_test1, y_test1, random_state = random.randint(10,50))\n",
    "            X_train, y_train = shuffle(X_train1, y_train1, random_state = random.randint(10,50))\n",
    "    \n",
    "            classes = np.unique(y_train)\n",
    "            le = LabelEncoder()\n",
    "            y_ind = le.fit_transform(y_train.ravel())\n",
    "            recip_freq = len(y_train) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n",
    "            class_weight = recip_freq[le.transform(classes)]\n",
    "    \n",
    "            \" One-hot coding\"\n",
    "            labels_train = to_categorical(y_train)\n",
    "            labels_test = to_categorical(y_test)\n",
    "\n",
    "            ###########################################################################\n",
    "            print(\"Ready to train\")\n",
    "\n",
    "            #######----------LSTM-----------START----######\n",
    "            ip = Input(shape=(sizeD_ts, sizeD_va), name='main_input')\n",
    "    \n",
    "            x = LSTM(32, return_sequences=True)(ip)\n",
    "            fea = LSTM(32)(x)\n",
    "\n",
    "            out = Dense(3, kernel_regularizer=regularizers.l2(0.001), activation='softmax')(fea)\n",
    "\n",
    "            model = Model(ip, out)\n",
    "            #######----------LSTM-----------END-----######\n",
    "            \n",
    "            epochs_s = 50\n",
    "            batch_s = 16\n",
    "            learning_rate = 1e-3   \n",
    "            weight_fn = \"./weights/weights_fea_LSTM\"+str(user)+\".h5\"\n",
    "            model_checkpoint = ModelCheckpoint(weight_fn, verbose=1, mode='max', monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "            stop = EarlyStopping(monitor='val_loss', patience=15)\n",
    "            callback_list = [model_checkpoint, stop]\n",
    "            optm = Adam(lr=learning_rate)\n",
    "\n",
    "            model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.fit(X_train, labels_train, batch_size=batch_s, epochs=epochs_s, callbacks = callback_list, class_weight=class_weight, verbose=2, validation_data=(X_test, labels_test))\n",
    "\n",
    "            model.load_weights(weight_fn)\n",
    "            prediction = model.predict(X_test);\n",
    "            y_pred = np.argmax(prediction, axis=1)\n",
    "    \n",
    "            cr = classification_report(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "            print(\"-------------------- Full NN -----------------------\")\n",
    "            print(\"Skill: \", skill)\n",
    "            print(\"Trial out:\", user)\n",
    "            acc = np.sum(y_pred == y_test)/y_pred.shape[0]\n",
    "            acc_1.append(acc)\n",
    "            print('Accuracy: ' + str(acc))\n",
    "            print(cr)\n",
    "            print(cm)\n",
    "            f = open(full_output_path, 'a+')\n",
    "            f.write(\"-------------------- Full NN -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr, cm))\n",
    "            \n",
    "            ##############################################\n",
    "            moFea = Model(ip, fea)\n",
    "            X_train_fea = moFea.predict(X_train)\n",
    "            X_test_fea = moFea.predict(X_test)\n",
    "            svm1 = svm.SVC(kernel = 'linear')\n",
    "            svm1.fit(X_train_fea, y_train)\n",
    "            prediction_fea = svm1.predict(X_test_fea)\n",
    "            y_pred_fea = prediction_fea\n",
    "            cr_fea = classification_report(y_test, y_pred_fea)\n",
    "            cm_fea = confusion_matrix(y_test, y_pred_fea)\n",
    "    \n",
    "            acc_fea = np.sum(y_pred_fea == y_test)/y_pred_fea.shape[0]\n",
    "            acc_1_fea.append(acc_fea)\n",
    "            print('Accuracy: ' + str(acc_fea))\n",
    "            print(cr_fea)\n",
    "            print(cm_fea)\n",
    "            f.write(\"-------------------- NN Fea -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc_fea))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr_fea, cm_fea))\n",
    "        for i in range(0,5):\n",
    "            accu += acc_1[i]\n",
    "            accu_fea += acc_1_fea[i]\n",
    "        accu = accu/5\n",
    "        accu_fea = accu_fea/5\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.write(\"Acc_full = \" + str(accu))\n",
    "        f.write(\"\\nAcc_fea = \" + str(accu_fea))\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three user groups (Beginner, Intermediate, Expert) are processed\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['../data/augment_data_from_csvs/',\n",
    "          '../data/augment_data_from_csvs/no_interm/',\n",
    "]\n",
    "\n",
    "outputs = ['../results/csv_based/',\n",
    "           '../results/csv_without_interms/',\n",
    "]\n",
    "\n",
    "ind = 1\n",
    "\n",
    "withoutInterms = (ind == 1)\n",
    "\n",
    "output_root = outputs[ind]\n",
    "input_root = inputs[ind]\n",
    "\n",
    "if withoutInterms:\n",
    "    print('Intermediate user data will be skipped')\n",
    "else:\n",
    "    print('All three user groups (Beginner, Intermediate, Expert) are processed')\n",
    "    \n",
    "groupRange = [0,2]\n",
    "if not withoutInterms:\n",
    "    groupRange = [0,1,2]\n",
    "print(groupRange)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "generateOnlyMissingResultFiles = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/csv_based/CNN/1_Knot_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/2_Knot_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/3_Knot_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/4_Knot_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/5_Knot_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/1_Needle_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/2_Needle_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/3_Needle_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/4_Needle_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/5_Needle_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/1_Suture_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/2_Suture_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/3_Suture_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/4_Suture_150_60_report_CNN_fea.txt already exists\n",
      "../results/csv_based/CNN/5_Suture_150_60_report_CNN_fea.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Mar 17 18:55:38 2019\n",
    "\n",
    "@author: xngu0004\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, BatchNormalization, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#####################################################\n",
    "\"Select a configuration\"\n",
    "\n",
    "list_skill = [ \"Knot\", \"Needle\", \"Suture\"]\n",
    "win = 150#[90, 120, 150, 180]\n",
    "over = 60#[30, 60]\n",
    "\n",
    "####################################################\n",
    "for skill in list_skill:\n",
    "    for tim in range(1,6):\n",
    "        full_output_path = output_root + 'CNN/' + str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+'_report_CNN_fea.txt'\n",
    "        if generateOnlyMissingResultFiles and path.exists(full_output_path):\n",
    "            print(full_output_path + ' already exists')\n",
    "            continue\n",
    "        acc_1 = []\n",
    "        accu = 0\n",
    "        acc_1_fea = []\n",
    "        accu_fea = 0\n",
    "        # users in Group\n",
    "        novice = ['B', 'G', 'H', 'I']\n",
    "        intermediate = ['C', 'F']\n",
    "        expert = ['D', 'E']\n",
    "\n",
    "        \" Load the data\"\n",
    "        \" Reading the data \"\n",
    "        print('loading the dataset')\n",
    "        dataset = sio.loadmat(input_root+skill+'_dataX_basic_'+str(win)+'_'+str(over)+'.mat')\n",
    "\n",
    "        features = dataset['dataX']\n",
    "        sizeD_ts = (((features[0,0:1])[0])[0,:])[0].shape[0]\n",
    "        sizeD_va = (((features[0,0:1])[0])[0,:])[0].shape[1]\n",
    "        print('loaded the dataset')\n",
    "    \n",
    "        ###############################################################################\n",
    "\n",
    "        print('Pre-process data')\n",
    "        for user in range(1,6):\n",
    "\n",
    "            \" Initialize data for concatenation \"\n",
    "            X_test1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_test1 = np.ones(1)\n",
    "            X_train1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_train1 = np.ones(1)\n",
    "\n",
    "            # Extract data for training and testing\n",
    "            for j in groupRange:\n",
    "                for i in range(0, (((features[0,j:j+1])[0])[0,:]).shape[0]):\n",
    "                    k = (((features[0,j:j+1])[0])[2,:])[i]\n",
    "                    if (k == user): # take out the ith trial of each user for testing\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_test1 = np.concatenate((X_test1, temp_x1), axis=0) \n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        \n",
    "                        if(user_temp in novice):\n",
    "                            y_test1 = np.concatenate((y_test1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_test1 = np.concatenate((y_test1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_test1 = np.concatenate((y_test1,[2]))\n",
    "                    else:\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_train1 = np.concatenate((X_train1, temp_x1), axis=0)\n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        \n",
    "                        if(user_temp in novice):\n",
    "                            y_train1 = np.concatenate((y_train1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_train1 = np.concatenate((y_train1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_train1 = np.concatenate((y_train1,[2]))\n",
    "            \n",
    "            \" Delete the first column \"\n",
    "            X_test1 = np.delete(X_test1,np.s_[0:1], axis=0)\n",
    "            y_test1 = np.delete(y_test1,np.s_[0:1])\n",
    "            X_train1 = np.delete(X_train1,np.s_[0:1], axis=0)\n",
    "            y_train1 = np.delete(y_train1,np.s_[0:1])\n",
    "\n",
    "            \" Shuffle the training and testing data \"\n",
    "            X_test, y_test = shuffle(X_test1, y_test1, random_state = random.randint(10,50))\n",
    "            X_train, y_train = shuffle(X_train1, y_train1, random_state = random.randint(10,50))\n",
    "    \n",
    "            classes = np.unique(y_train)\n",
    "            le = LabelEncoder()\n",
    "            y_ind = le.fit_transform(y_train.ravel())\n",
    "            recip_freq = len(y_train) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n",
    "            class_weight = recip_freq[le.transform(classes)]\n",
    "    \n",
    "            \" One-hot coding\"\n",
    "            labels_train = to_categorical(y_train)\n",
    "            labels_test = to_categorical(y_test)\n",
    "\n",
    "            ###########################################################################\n",
    "            print(\"Ready to train\")\n",
    "            #model = model_arch2(withOP=True)\n",
    "            \n",
    "            #######-------- CNN -------------START----#####\n",
    "            ip = Input(shape=(sizeD_ts, sizeD_va), name='main_input')\n",
    "    \n",
    "            x = Conv1D(16, 7, padding='same', kernel_initializer='he_uniform')(ip)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "\n",
    "            x = Conv1D(32, 5, padding='same', kernel_initializer='he_uniform')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            fea = GlobalAveragePooling1D()(x)\n",
    "\n",
    "            out = Dense(3, kernel_regularizer=regularizers.l2(0.001), activation='softmax')(fea)\n",
    "\n",
    "            model = Model(ip, out)\n",
    "            #######-------- CNN -------------END----######\n",
    "            \n",
    "            epochs_s = 100\n",
    "            batch_s = 16\n",
    "            learning_rate = 1e-3   \n",
    "            weight_fn = \"./weights/weights_fea_CNNi\"+str(user)+\".h5\"\n",
    "            model_checkpoint = ModelCheckpoint(weight_fn, verbose=1, mode='max', monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "            stop = EarlyStopping(monitor='val_loss', patience=15)\n",
    "            callback_list = [model_checkpoint, stop]\n",
    "            optm = Adam(lr=learning_rate)\n",
    "\n",
    "            model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            history = model.fit(X_train, labels_train, batch_size=batch_s, epochs=epochs_s, callbacks = callback_list, class_weight=class_weight, verbose=2, validation_data=(X_test, labels_test))\n",
    "            \n",
    "#            plt.plot(history.history['acc'])\n",
    "#            plt.plot(history.history['val_acc'])\n",
    "#            plt.title('model accuracy')\n",
    "#            plt.ylabel('accuracy')\n",
    "#            plt.xlabel('epoch')\n",
    "#            plt.legend(['train', 'test'], loc='upper left')\n",
    "#            plt.savefig(str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+\"_user_\"+str(user)+'acc.pdf')\n",
    "#            plt.close()\n",
    "#            \n",
    "#            plt.plot(history.history['loss'])\n",
    "#            plt.plot(history.history['val_loss'])\n",
    "#            plt.title('model loss')\n",
    "#            plt.ylabel('loss')\n",
    "#            plt.xlabel('epoch')\n",
    "#            plt.legend(['train', 'test'], loc='upper left')\n",
    "#            plt.savefig(str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+\"_user_\"+str(user)+'lost.pdf')\n",
    "#            plt.close()\n",
    "            \n",
    "            model.load_weights(weight_fn)\n",
    "            prediction = model.predict(X_test);\n",
    "            y_pred = np.argmax(prediction, axis=1)\n",
    "    \n",
    "            cr = classification_report(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "            print(\"-------------------- Full NN -----------------------\")\n",
    "            print(\"Skill: \", skill)\n",
    "            print(\"Trial out:\", user)\n",
    "            acc = np.sum(y_pred == y_test)/y_pred.shape[0]\n",
    "            acc_1.append(acc)\n",
    "            print('Accuracy: ' + str(acc))\n",
    "            print(cr)\n",
    "            print(cm)\n",
    "            f = open(full_output_path, 'a+')\n",
    "            f.write(\"-------------------- Full NN -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr, cm))\n",
    "            \n",
    "            ##############################################\n",
    "            moFea = Model(ip, fea)\n",
    "            X_train_fea = moFea.predict(X_train)\n",
    "            X_test_fea = moFea.predict(X_test)\n",
    "            svm1 = svm.SVC(kernel = 'linear')\n",
    "            svm1.fit(X_train_fea, y_train)\n",
    "            prediction_fea = svm1.predict(X_test_fea)\n",
    "            \n",
    "            y_pred_fea = prediction_fea\n",
    "            cr_fea = classification_report(y_test, y_pred_fea)\n",
    "            cm_fea = confusion_matrix(y_test, y_pred_fea)\n",
    "    \n",
    "            acc_fea = np.sum(y_pred_fea == y_test)/y_pred_fea.shape[0]\n",
    "            acc_1_fea.append(acc_fea)\n",
    "            print('Accuracy: ' + str(acc_fea))\n",
    "            print(cr_fea)\n",
    "            print(cm_fea)\n",
    "            f.write(\"-------------------- NN Fea -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc_fea))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr_fea, cm_fea))\n",
    "        for i in range(0,5):\n",
    "            accu += acc_1[i]\n",
    "            accu_fea += acc_1_fea[i]\n",
    "        accu = accu/5\n",
    "        accu_fea = accu_fea/5\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.write(\"Acc_full = \" + str(accu))\n",
    "        f.write(\"\\nAcc_fea = \" + str(accu_fea))\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three user groups (Beginner, Intermediate, Expert) are processed\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['../data/augment_data_from_csvs/',\n",
    "          '../data/augment_data_from_csvs/no_interm/',\n",
    "]\n",
    "\n",
    "outputs = ['../results/csv_based/',\n",
    "           '../results/csv_without_interms/',\n",
    "]\n",
    "\n",
    "ind = 1\n",
    "\n",
    "withoutInterms = (ind == 1)\n",
    "\n",
    "output_root = outputs[ind]\n",
    "input_root = inputs[ind]\n",
    "\n",
    "if withoutInterms:\n",
    "    print('Intermediate user data will be skipped')\n",
    "else:\n",
    "    print('All three user groups (Beginner, Intermediate, Expert) are processed')\n",
    "    \n",
    "groupRange = [0,2]\n",
    "if not withoutInterms:\n",
    "    groupRange = [0,1,2]\n",
    "print(groupRange)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "generateOnlyMissingResultFiles = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_based/CNN+LSTM/1_Suture_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/2_Suture_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/3_Suture_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/4_Suture_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/5_Suture_180_30_report_CNNLSTM_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_based/CNN+LSTM/1_Knot_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/2_Knot_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/3_Knot_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/4_Knot_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/5_Knot_180_30_report_CNNLSTM_fea.txt already exists\n",
      "loading the dataset\n",
      "loaded the dataset\n",
      "../results/csv_based/CNN+LSTM/1_Needle_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/2_Needle_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/3_Needle_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/4_Needle_180_30_report_CNNLSTM_fea.txt already exists\n",
      "../results/csv_based/CNN+LSTM/5_Needle_180_30_report_CNNLSTM_fea.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Mar 17 13:21:11 2019\n",
    "\n",
    "@author: xngu0004\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, BatchNormalization, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import svm\n",
    "\n",
    "#####################################################\n",
    "\"Select a configuration\"\n",
    "\n",
    "list_skill = [\"Suture\",\"Knot\",\"Needle\"]\n",
    "win = 180#[90,120,150,180]\n",
    "over = 30#[30,60]\n",
    "\n",
    "# users in Group\n",
    "novice = ['B', 'G', 'H', 'I']\n",
    "intermediate = ['C', 'F']\n",
    "expert = ['D', 'E']\n",
    "####################################################\n",
    "for skill in list_skill:\n",
    "    \" Load the data\"\n",
    "    print('loading the dataset')\n",
    "    dataset = sio.loadmat(input_root+skill+'_dataX_basic_'+str(win)+'_'+str(over)+'.mat')\n",
    "    features = dataset['dataX']\n",
    "    sizeD_ts = (((features[0,0:1])[0])[0,:])[0].shape[0]\n",
    "    sizeD_va = (((features[0,0:1])[0])[0,:])[0].shape[1]\n",
    "    print('loaded the dataset')\n",
    "    \n",
    "    for tim in range(1,6):\n",
    "        full_output_path = output_root + 'CNN+LSTM/' + str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+'_report_CNNLSTM_fea.txt'\n",
    "        if generateOnlyMissingResultFiles and path.exists(full_output_path):\n",
    "            print(full_output_path + ' already exists')\n",
    "            continue\n",
    "        acc_1 = []\n",
    "        accu = 0\n",
    "        acc_1_fea = []\n",
    "        accu_fea = 0\n",
    "\n",
    "        ##################################################\n",
    "        print('Pre-process data')\n",
    "        for user in range(1,6):\n",
    "\n",
    "            \" Initialize data for concatenation \"\n",
    "            X_test1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_test1 = np.ones(1)\n",
    "            X_train1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_train1 = np.ones(1)\n",
    "\n",
    "            # Extract data for training and testing\n",
    "            for j in groupRange:\n",
    "                for i in range(0, (((features[0,j:j+1])[0])[0,:]).shape[0]):\n",
    "                    k = (((features[0,j:j+1])[0])[2,:])[i]\n",
    "                    if (k == user): # take out the ith trial of each user for testing\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_test1 = np.concatenate((X_test1, temp_x1), axis=0) \n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        if(user_temp in novice):\n",
    "                            y_test1 = np.concatenate((y_test1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_test1 = np.concatenate((y_test1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_test1 = np.concatenate((y_test1,[2]))\n",
    "                    else:\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_train1 = np.concatenate((X_train1, temp_x1), axis=0)\n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        \n",
    "                        if(user_temp in novice):\n",
    "                            y_train1 = np.concatenate((y_train1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_train1 = np.concatenate((y_train1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_train1 = np.concatenate((y_train1,[2]))\n",
    "        \n",
    "            \" Delete the first column \"\n",
    "            X_test1 = np.delete(X_test1,np.s_[0:1], axis=0)\n",
    "            y_test1 = np.delete(y_test1,np.s_[0:1])\n",
    "            X_train1 = np.delete(X_train1,np.s_[0:1], axis=0)\n",
    "            y_train1 = np.delete(y_train1,np.s_[0:1])\n",
    "            print(\"Data shuffle\")\n",
    "            \" Shuffle the training and testing data \"\n",
    "            X_test, y_test = shuffle(X_test1, y_test1, random_state = random.randint(10,50))\n",
    "            X_train, y_train = shuffle(X_train1, y_train1, random_state = random.randint(10,50))\n",
    "    \n",
    "            classes = np.unique(y_train)\n",
    "            le = LabelEncoder()\n",
    "            y_ind = le.fit_transform(y_train.ravel())\n",
    "            recip_freq = len(y_train) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n",
    "            class_weight = recip_freq[le.transform(classes)]\n",
    "    \n",
    "            \" One-hot coding\"\n",
    "            labels_train = to_categorical(y_train)\n",
    "            labels_test = to_categorical(y_test)\n",
    "\n",
    "            ###########################################################################\n",
    "            print(\"Ready to train\")\n",
    "            #######---------CNN+LSTM------------START---#####\n",
    "            ip = Input(shape=(sizeD_ts, sizeD_va), name='main_input')\n",
    "    \n",
    "            x = Conv1D(16, 7, padding='same', kernel_initializer='he_uniform')(ip)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "\n",
    "            x = Conv1D(32, 5, padding='same', kernel_initializer='he_uniform')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "    \n",
    "            x = LSTM(32, return_sequences=True)(x)\n",
    "            fea = LSTM(32)(x)\n",
    "\n",
    "            out = Dense(3, kernel_regularizer=regularizers.l2(0.001), activation='softmax')(fea)\n",
    "            \n",
    "            model = Model(ip, out)\n",
    "            \n",
    "            #######------CNN+LSTM----------END----#####\n",
    "            \n",
    "            epochs_s = 100\n",
    "            batch_s = 16\n",
    "            learning_rate = 1e-3   \n",
    "            weight_fn = \"./weights/weights_fea_CNNLSTM_\"+str(user)+\".h5\"\n",
    "            model_checkpoint = ModelCheckpoint(weight_fn, verbose=1, mode='max', monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "            stop = EarlyStopping(monitor='val_loss', patience=15)\n",
    "            callback_list = [model_checkpoint, stop]\n",
    "            optm = Adam(lr=learning_rate)\n",
    "\n",
    "            model.compile(optimizer=optm, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.fit(X_train, labels_train, batch_size=batch_s, epochs=epochs_s, callbacks = callback_list, class_weight=class_weight, verbose=2, validation_data=(X_test, labels_test))\n",
    "\n",
    "            model.load_weights(weight_fn)\n",
    "            prediction = model.predict(X_test);\n",
    "            y_pred = np.argmax(prediction, axis=1)\n",
    "    \n",
    "            cr = classification_report(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "            print(\"-------------------- Full NN -----------------------\")\n",
    "            print(\"Skill: \", skill)\n",
    "            print(\"Trial out:\", user)\n",
    "            acc = np.sum(y_pred == y_test)/y_pred.shape[0]\n",
    "            acc_1.append(acc)\n",
    "            print('Accuracy: ' + str(acc))\n",
    "            print(cr)\n",
    "            print(cm)\n",
    "            f = open(full_output_path, 'a+')\n",
    "            f.write(\"-------------------- Full NN -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr, cm))\n",
    "            \n",
    "            ##############################################\n",
    "            moFea = Model(ip, fea)\n",
    "            X_train_fea = moFea.predict(X_train)\n",
    "            X_test_fea = moFea.predict(X_test)\n",
    "            svm1 = svm.SVC(kernel = 'linear')\n",
    "            svm1.fit(X_train_fea, y_train)\n",
    "            prediction_fea = svm1.predict(X_test_fea)\n",
    "            y_pred_fea = prediction_fea\n",
    "            cr_fea = classification_report(y_test, y_pred_fea)\n",
    "            cm_fea = confusion_matrix(y_test, y_pred_fea)\n",
    "    \n",
    "            acc_fea = np.sum(y_pred_fea == y_test)/y_pred_fea.shape[0]\n",
    "            acc_1_fea.append(acc_fea)\n",
    "            print('Accuracy: ' + str(acc_fea))\n",
    "            print(cr_fea)\n",
    "            print(cm_fea)\n",
    "            f.write(\"-------------------- NN Fea -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc_fea))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr_fea, cm_fea))\n",
    "        for i in range(0,5):\n",
    "            accu += acc_1[i]\n",
    "            accu_fea += acc_1_fea[i]\n",
    "        accu = accu/5\n",
    "        accu_fea = accu_fea/5\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.write(\"Acc_full = \" + str(accu))\n",
    "        f.write(\"\\nAcc_fea = \" + str(accu_fea))\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvAuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three user groups (Beginner, Intermediate, Expert) are processed\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['../data/augment_data_from_csvs/',\n",
    "          '../data/augment_data_from_csvs/no_interm/',\n",
    "]\n",
    "\n",
    "outputs = ['../results/csv_based/',\n",
    "           '../results/csv_without_interms/',\n",
    "]\n",
    "\n",
    "ind = 1\n",
    "\n",
    "withoutInterms = (ind == 1)\n",
    "\n",
    "output_root = outputs[ind]\n",
    "input_root = inputs[ind]\n",
    "\n",
    "if withoutInterms:\n",
    "    print('Intermediate user data will be skipped')\n",
    "else:\n",
    "    print('All three user groups (Beginner, Intermediate, Expert) are processed')\n",
    "    \n",
    "groupRange = [0,2]\n",
    "if not withoutInterms:\n",
    "    groupRange = [0,1,2]\n",
    "print(groupRange)\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "generateOnlyMissingResultFiles = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/csv_based/convAuto/1_Suture_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/2_Suture_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/3_Suture_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/4_Suture_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/5_Suture_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/1_Knot_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/2_Knot_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/3_Knot_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/4_Knot_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/5_Knot_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/1_Needle_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/2_Needle_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/3_Needle_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/4_Needle_120_60_report_AutoCNN_fea.txt already exists\n",
      "../results/csv_based/convAuto/5_Needle_120_60_report_AutoCNN_fea.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Apr  7 16:36:07 2019\n",
    "\n",
    "@author: xngu0004\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, BatchNormalization, Activation, Flatten\n",
    "from keras.layers import LSTM, Dense, MaxPooling1D, UpSampling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#####################################################\n",
    "\"Select a configuration\"\n",
    "\n",
    "list_skill = [\"Suture\", \"Knot\", \"Needle\"]\n",
    "win = 120#90, 120, 150, 180]\n",
    "over = 60#[30, 60]\n",
    "\n",
    "####################################################\n",
    "for skill in list_skill:\n",
    "    for tim in range(1,6):\n",
    "        full_output_path = output_root + 'convAuto/' + str(tim)+'_'+skill+'_'+str(win)+'_'+str(over)+'_report_AutoCNN_fea.txt'\n",
    "        if generateOnlyMissingResultFiles and path.exists(full_output_path):\n",
    "            print(full_output_path + ' already exists')\n",
    "            continue\n",
    "        acc_1 = []\n",
    "        accu = 0\n",
    "        acc_1_fea = []\n",
    "        accu_fea = 0\n",
    "        # users in Group\n",
    "        novice = ['B', 'G', 'H', 'I']\n",
    "        intermediate = ['C', 'F']\n",
    "        expert = ['D', 'E']\n",
    "\n",
    "        \" Load the data\"\n",
    "        \" Reading the data \"\n",
    "        print('loading the dataset')\n",
    "        dataset = sio.loadmat(input_root+skill+'_dataX_basic_'+str(win)+'_'+str(over)+'.mat')\n",
    "\n",
    "        features = dataset['dataX']\n",
    "        sizeD_ts = (((features[0,0:1])[0])[0,:])[0].shape[0]\n",
    "        sizeD_va = (((features[0,0:1])[0])[0,:])[0].shape[1]\n",
    "        print('loaded the dataset')\n",
    "    \n",
    "        ###############################################################################\n",
    "\n",
    "        print('Pre-process data')\n",
    "        for user in range(1,6):\n",
    "\n",
    "            \" Initialize data for concatenation \"\n",
    "            X_test1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_test1 = np.ones(1)\n",
    "            X_train1 = np.ones((1,sizeD_ts,sizeD_va))\n",
    "            y_train1 = np.ones(1)\n",
    "\n",
    "            # Extract data for training and testing\n",
    "            for j in groupRange:\n",
    "                upper_range = (((features[0,j:j+1])[0])[0,:]).shape[0]\n",
    "                for i in range(0, upper_range):\n",
    "                    k = (((features[0,j:j+1])[0])[2,:])[i]\n",
    "                    if (k == user): # take out the ith trial of each user for testing\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_test1 = np.concatenate((X_test1, temp_x1), axis=0) \n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        if(user_temp in novice):\n",
    "                            y_test1 = np.concatenate((y_test1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_test1 = np.concatenate((y_test1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_test1 = np.concatenate((y_test1,[2]))\n",
    "                    else:\n",
    "                        temp_x = (((features[0,j:j+1])[0])[0,:])[i]\n",
    "                        temp_x1 = temp_x.reshape(1,sizeD_ts,sizeD_va)\n",
    "                        X_train1 = np.concatenate((X_train1, temp_x1), axis=0)\n",
    "                        user_temp = (((features[0,j:j+1])[0])[1,:])[i]\n",
    "                        if(user_temp in novice):\n",
    "                            y_train1 = np.concatenate((y_train1,[0]))\n",
    "                        elif(user_temp in intermediate):\n",
    "                            y_train1 = np.concatenate((y_train1,[1]))\n",
    "                        elif(user_temp in expert):\n",
    "                            y_train1 = np.concatenate((y_train1,[2]))\n",
    "        \n",
    "            \" Delete the first column \"\n",
    "            X_test1 = np.delete(X_test1,np.s_[0:1], axis=0)\n",
    "            y_test1 = np.delete(y_test1,np.s_[0:1])\n",
    "            X_train1 = np.delete(X_train1,np.s_[0:1], axis=0)\n",
    "            y_train1 = np.delete(y_train1,np.s_[0:1])\n",
    "\n",
    "            \" Shuffle the training and testing data \"\n",
    "            X_test, y_test = shuffle(X_test1, y_test1, random_state = random.randint(10,50))\n",
    "            X_train, y_train = shuffle(X_train1, y_train1, random_state = random.randint(10,50))\n",
    "    \n",
    "            classes = np.unique(y_train)\n",
    "            le = LabelEncoder()\n",
    "            y_ind = le.fit_transform(y_train.ravel())\n",
    "            recip_freq = len(y_train) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n",
    "            class_weight = recip_freq[le.transform(classes)]\n",
    "    \n",
    "            \" One-hot coding\"\n",
    "            labels_train = to_categorical(y_train)\n",
    "            labels_test = to_categorical(y_test)\n",
    "\n",
    "            ###########################################################################\n",
    "            print(\"Ready to train\")\n",
    "\n",
    "            #######-------- auto - CNN -------------        \n",
    "            \n",
    "            ip = Input(shape=(sizeD_ts, sizeD_va), name='main_input')\n",
    "    \n",
    "            x = Conv1D(32, 7, activation='relu', padding='same', kernel_initializer='he_uniform')(ip)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = MaxPooling1D(3)(x)\n",
    "            \n",
    "            x = Conv1D(16, 5, activation='relu',padding='same', kernel_initializer='he_uniform')(x)\n",
    "            z = BatchNormalization()(x)\n",
    "            x = MaxPooling1D(2)(x)\n",
    "\n",
    "            x = Conv1D(16, 3, activation='relu',padding='same', kernel_initializer='he_uniform')(x)\n",
    "            z = BatchNormalization()(x)\n",
    "            \n",
    "            fea = Flatten()(z)\n",
    "\n",
    "            z = Conv1D(16, 3, activation='relu',padding='same', kernel_initializer='he_uniform')(z)\n",
    "            z = BatchNormalization()(z)\n",
    "            z = UpSampling1D(2)(z)\n",
    "            \n",
    "            z = Conv1D(16, 5, activation='relu',padding='same', kernel_initializer='he_uniform')(z)\n",
    "            z = BatchNormalization()(z)\n",
    "            z = UpSampling1D(3)(z)\n",
    "            \n",
    "            z = Conv1D(32, 7, activation='relu', padding='same', kernel_initializer='he_uniform')(z)\n",
    "            z = BatchNormalization()(z)\n",
    "\n",
    "            #out = Conv1D(76, 5, activation='sigmoid', padding='same', kernel_initializer='he_uniform')(z)\n",
    "            out = Conv1D(240, 5, activation='sigmoid', padding='same', kernel_initializer='he_uniform')(z)\n",
    "\n",
    "            model = Model(ip, out)\n",
    "\n",
    "#######---------------------\n",
    "            \n",
    "            epochs_s = 100\n",
    "            batch_s = 16\n",
    "            learning_rate = 1e-3   \n",
    "            weight_fn = \"./weights/weights_fea_autoCNN_\"+str(user)+\".h5\"\n",
    "            model_checkpoint = ModelCheckpoint(weight_fn, verbose=1, mode='max', monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "            stop = EarlyStopping(monitor='val_loss', patience=15)\n",
    "            callback_list = [model_checkpoint, stop]\n",
    "            optm = Adam(lr=learning_rate)\n",
    "\n",
    "            model.compile(optimizer=optm, loss='mean_squared_error', metrics=['accuracy'])\n",
    "            model.fit(X_train, X_train, batch_size=batch_s, epochs=epochs_s, callbacks = callback_list, class_weight=class_weight, verbose=2, validation_data=(X_test, X_test))\n",
    "\n",
    "            model.load_weights(weight_fn)\n",
    "            prediction = model.predict(X_test);\n",
    "            print(\"Skill: \", skill)\n",
    "            print(\"Trial out:\", user)\n",
    "            f = open(full_output_path, 'a+')\n",
    "            moFea = Model(ip, fea)\n",
    "            X_train_fea = moFea.predict(X_train)\n",
    "            X_test_fea = moFea.predict(X_test)\n",
    "            \n",
    "            svm1 = svm.SVC(kernel = 'linear')\n",
    "            svm1.fit(X_train_fea, y_train)\n",
    "            prediction_fea = svm1.predict(X_test_fea)\n",
    "            \n",
    "            y_pred_fea = prediction_fea\n",
    "            cr_fea = classification_report(y_test, y_pred_fea)\n",
    "            cm_fea = confusion_matrix(y_test, y_pred_fea)\n",
    "            acc_fea = np.sum(y_pred_fea == y_test)/y_pred_fea.shape[0]\n",
    "            acc_1_fea.append(acc_fea)\n",
    "            print(\"-------------------- NN Fea -----------------------\")\n",
    "            print('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            print('Accuracy: ' + str(acc_fea))\n",
    "            print(cr_fea)\n",
    "            print(cm_fea)\n",
    "            f.write(\"-------------------- NN Fea -----------------------\")\n",
    "            f.write('---------' + skill + \" Win: \" + str(win) + \" Shift: \" + str(over) + \" Trial: \" + str(user) + '---------\\n\\n')\n",
    "            f.write('Accuracy: ' + str(acc_fea))\n",
    "            f.write('\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\n'.format(cr_fea, cm_fea))\n",
    "        for i in range(0,5):\n",
    "            accu_fea += acc_1_fea[i]\n",
    "        accu_fea = accu_fea/5\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.write(\"\\nAcc_fea = \" + str(accu_fea))\n",
    "        f.write(\"\\n-------------------------------------------\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
